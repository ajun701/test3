# app.py
# -*- coding: utf-8 -*-
"""
å çº¸å¿ƒæ„æ——èˆ°åº—ï½œé€€è¿è´¹æ™ºèƒ½å®¡æ ¸ç³»ç»Ÿ Web App
æŠ€æœ¯æ ˆï¼šStreamlit + Pandas + DashScopeï¼ˆé€šä¹‰åƒé—®-VLï¼‰

Tab1ï¼šç­ç‰›ã€Šé€€è¿è´¹è‡ªåŠ©ç™»è®°è¡¨ã€‹æ¸…æ´— + è§„åˆ™åˆç­›ï¼ˆé‡‘é¢/è´¦å·/å®å/ç‰©æµå•å·ï¼‰
Tab2ï¼šæ­¥éª¤äºŒå…¥åº“åŒ¹é…ï¼ˆæ­£å¸¸è¡¨ + å·²å…¥åº“ç‰©æµå•å·è¡¨ï¼‰ + æ­¥éª¤ä¸‰AI è§†è§‰å¤æ ¸ï¼ˆå¤šå›¾è¯†åˆ«ï¼‰

ã€å…³é”®é˜²å‘ç‚¹ã€‘
1) è¯»å–åç¬¬ä¸€æ—¶é—´ df.columns stripï¼ˆå¿…é¡»ï¼‰
2) st.dataframe ä¸æŒ‡å®šåˆ—åï¼Œé¿å… KeyError
3) ç¼ºå°‘å¿…è¦åˆ— st.error å‹å¥½æç¤º
4) Excel è¶…é“¾æ¥ï¼špandas é»˜è®¤è¯»ä¸åˆ° hyperlink.target
   - è¯»å–æ—¶ç”¨ openpyxl æŠ½å– URLï¼ˆå…¼å®¹ï¼šè¶…é“¾æ¥å¯¹è±¡ / HYPERLINK() å…¬å¼ / tooltip / æ‰¹æ³¨ï¼‰
     å­˜å…¥è¾…åŠ©åˆ—ï¼š{åˆ—å}__hyperlinkï¼ˆå¹¶æŒ‰ df è¡Œæ•°å¯¹é½ï¼‰
   - å¯¼å‡ºæ—¶ç”¨ openpyxl å†™å› hyperlinkï¼ˆä¿ç•™åŸæ–‡å­—ï¼Œå¦‚â€œé¢„è§ˆ/æµè§ˆâ€ï¼Œä½†å¯ç‚¹å‡»ï¼‰
5) é¢„è§ˆé“¾æ¥ï¼ˆexportFilePreview?url=...__...ï¼‰åŒ…å«å¤šå¼ å›¾ï¼šAI é˜¶æ®µä¼šæ‹†å‡ºå¤šå¼ å›¾ç‰‡ä¸€èµ·é€æ¨¡å‹æŸ¥æ‰¾è¿è´¹æˆªå›¾
"""

import os
import re
import json
import time
import math
import zipfile
import threading
import traceback
from collections import Counter
from functools import lru_cache
from io import BytesIO
from datetime import datetime
from decimal import Decimal, InvalidOperation
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any
from urllib.parse import unquote, urlparse, parse_qs

import pandas as pd
import streamlit as st
from openpyxl import load_workbook

# ====== DashScopeï¼ˆé˜¿é‡Œäº‘ç™¾ç‚¼/é€šä¹‰åƒé—®ï¼‰======
try:
    import dashscope
    from http import HTTPStatus
except Exception:
    dashscope = None
    HTTPStatus = None


# =============================================================================
# ã€1ã€‘å…¨å±€å¯é…ç½®å˜é‡ï¼ˆåˆ—åå˜åŒ–åªæ”¹è¿™é‡Œï¼‰
# =============================================================================

COL_AMOUNT_CANDIDATES = [
    "*å¯„å›å¿«é€’å®ä»˜é‡‘é¢", "å¯„å›å¿«é€’å®ä»˜é‡‘é¢", "*å¯„å›è¿è´¹é‡‘é¢", "å¯„å›è¿è´¹é‡‘é¢", "é€€å›è¿è´¹é‡‘é¢", "*é€€å›è¿è´¹é‡‘é¢"
]
COL_ALIPAY_ACCOUNT_CANDIDATES = [
    "*é€€è¿è´¹çš„æ”¯ä»˜å®è´¦å·", "é€€è¿è´¹çš„æ”¯ä»˜å®è´¦å·", "æ”¯ä»˜å®è´¦å·", "æ”¶æ¬¾æ”¯ä»˜å®è´¦å·", "æ”¯ä»˜å®æ”¶æ¬¾è´¦å·"
]
COL_ALIPAY_NAME_CANDIDATES = [
    "*é€€è¿è´¹çš„æ”¯ä»˜å®å®å", "é€€è¿è´¹çš„æ”¯ä»˜å®å®å", "æ”¯ä»˜å®å®å", "æ”¶æ¬¾äººå§“å", "æ”¶æ¬¾äºº"
]
COL_LOGISTICS_NO_CANDIDATES = [
    "*å¯„å›æ¢è´§å¿«é€’å•å·", "å¯„å›æ¢è´§å¿«é€’å•å·", "*é€€å›ç‰©æµå•å·", "é€€å›ç‰©æµå•å·", "å¯„å›ç‰©æµå•å·", "å¿«é€’å•å·"
]
COL_SCREENSHOT_CANDIDATES = [
    "*å•†å“ç‘•ç–µ+é‡‘é¢æˆªå›¾", "å•†å“ç‘•ç–µ+é‡‘é¢æˆªå›¾", "å¯„å›è¿è´¹æˆªå›¾", "è¿è´¹æˆªå›¾", "æˆªå›¾", "å›¾ç‰‡URL", "å›¾ç‰‡é“¾æ¥"
]
COL_ID_CANDIDATES = [
    "ID", "id", "*ID", "æ—ºæ—ºID", "*æ—ºæ—ºID", "ç”¨æˆ·ID", "ä¹°å®¶ID", "ä¼šå‘˜ID"
]
COL_ORDER_NO_CANDIDATES = [
    "è®¢å•å·", "*è®¢å•å·", "è®¢å•ç¼–å·", "ä¸»è®¢å•å·", "å­è®¢å•å·", "å¤šç¬”è®¢å•å·",
    "è®¢å•å·ï¼ˆå¤šç¬”è®¢å•åˆ†å¼€æäº¤ï¼‰", "*è®¢å•å·ï¼ˆå¤šç¬”è®¢å•åˆ†å¼€æäº¤ï¼‰"
]

MAX_REFUND_AMOUNT = 12.0

REGEX_PHONE = re.compile(r"^1[3-9]\d{9}$")
REGEX_EMAIL = re.compile(r"^[A-Za-z0-9_.+-]+@[A-Za-z0-9-]+\.[A-Za-z0-9-.]+$")

# âœ… è°ƒæ•´ï¼šå®å 2~5 ä¸ªæ±‰å­—
REGEX_CN_NAME = re.compile(r"^[\u4e00-\u9fa5]{2,5}$")

# âœ… è°ƒæ•´ï¼šç‰©æµå•å· 10~16 ä½å­—æ¯æ•°å­—ï¼Œä¸”å¿…é¡»åŒ…å«æ•°å­—
REGEX_LOGISTICS = re.compile(r"^(?=.*\d)[A-Za-z0-9]{10,16}$")
REGEX_MONEY_CLEAN = re.compile(r"[^0-9.\-]")
REGEX_NON_ALNUM = re.compile(r"[^A-Za-z0-9]")
REGEX_URL_IN_PARENS = re.compile(r"\((https?://[^\s)]+)\)")
REGEX_URL_GENERIC = re.compile(r"(https?://[^\s\]\"')]+)")
REGEX_PREVIEW_SPLIT = re.compile(r"__|;|\s+")
REGEX_SCI_NUMBER = re.compile(r"^[+-]?(?:\d+\.?\d*|\.\d+)[eE][+-]?\d+$")
REGEX_EXCEL_HYPERLINK_FORMULA = re.compile(r'HYPERLINK\(\s*"([^"]+)"\s*[,;]\s*', re.IGNORECASE)
REGEX_EXCEL_URL_FALLBACK = re.compile(r"(https?://[^\s\"')]+)")
IMAGE_EXTENSIONS = (".jpg", ".jpeg", ".png", ".webp", ".bmp")
IDENTIFIER_COLUMN_KEYWORDS = (
    "è®¢å•", "å•å·", "ç‰©æµ", "å¿«é€’", "è´¦å·", "æ”¯ä»˜å®", "æµæ°´", "ç¼–å·", "ID", "id"
)

COL_ABNORMAL_REASON = "å¼‚å¸¸åŸå› "
COL_AI_EXTRACTED_AMOUNT = "AIæå–è¿è´¹é‡‘é¢"
COL_AI_MATCH = "AIæ˜¯å¦ä¸€è‡´"
COL_AI_NOTE = "AIå¼‚å¸¸è¯´æ˜"

# âœ… å…¥åº“åŒ¹é…æ–°å¢åˆ—
COL_INBOUND_FLAG = "æ˜¯å¦å·²å…¥åº“"     # å€¼ï¼šå·²å…¥åº“ / ç©º
COL_INBOUND_NOTE = "å…¥åº“åŒ¹é…è¯´æ˜"   # å€¼ï¼šåŒ¹é…åˆ°å·²å…¥åº“è¡¨ / ç©º

DEFAULT_VL_MODEL = "qwen-vl-plus"
PROGRESS_UPDATE_EVERY = 1

# Excel è¶…é“¾æ¥è¾…åŠ©åˆ—åç¼€ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼Œå¯¼å‡ºä¸ä¼šå¸¦å‡ºå»ï¼‰
HYPERLINK_SUFFIX = "__hyperlink"
HISTORY_FILE_NAME = "operation_history.jsonl"
ARTIFACT_DIR_NAME = "operation_artifacts"
TASK_DIR_NAME = "operation_tasks"

COL_AI_TASK_STATUS = "AIä»»åŠ¡çŠ¶æ€"
COL_AI_TASK_UPDATED_AT = "AIå¤„ç†æ—¶é—´"

AI_TASK_STATUS_RUNNING = "running"
AI_TASK_STATUS_PAUSED = "paused"
AI_TASK_STATUS_COMPLETED = "completed"
AI_TASK_STATUS_ERROR = "error"

AI_ROW_STATUS_PENDING = "å¾…å¤„ç†"
AI_ROW_STATUS_DONE = "å·²å¤„ç†"
AI_ROW_STATUS_OUT_OF_SCOPE = "è¶…å‡ºæœ¬æ¬¡å¤„ç†ä¸Šé™"

_AI_TASK_FILE_LOCK = threading.Lock()


# =============================================================================
# ã€2ã€‘åŸºç¡€å·¥å…·å‡½æ•°ï¼ˆè¯»å–ã€æ¸…æ´—ã€æ ¡éªŒï¼‰
# =============================================================================

def now_ts() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def now_iso() -> str:
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def get_history_file_path() -> Path:
    return Path.cwd() / HISTORY_FILE_NAME


def get_artifact_root_path() -> Path:
    root = Path.cwd() / ARTIFACT_DIR_NAME
    root.mkdir(parents=True, exist_ok=True)
    return root


def _sanitize_file_name(name: str) -> str:
    safe = re.sub(r'[<>:"/\\|?*\x00-\x1F]+', "_", str(name)).strip()
    return safe[:180] if safe else f"{now_ts()}_unnamed.xlsx"


def _extract_display_name_from_artifact_name(name: str) -> str:
    parts = str(name).split("__", 2)
    return parts[2] if len(parts) == 3 else str(name)


def save_artifact_bytes(stage_key: str, file_name: str, data: bytes) -> str:
    if not data:
        return ""
    now = datetime.now()
    date_dir = get_artifact_root_path() / now.strftime("%Y") / now.strftime("%m") / now.strftime("%d")
    date_dir.mkdir(parents=True, exist_ok=True)

    safe_stage = re.sub(r"[^A-Za-z0-9_-]+", "_", str(stage_key)).strip("_") or "stage"
    safe_file = _sanitize_file_name(file_name)
    ts = now.strftime("%Y%m%d_%H%M%S")

    candidate = date_dir / f"{ts}__{safe_stage}__{safe_file}"
    idx = 1
    stem = Path(safe_file).stem
    suffix = Path(safe_file).suffix
    while candidate.exists():
        candidate = date_dir / f"{ts}__{safe_stage}__{stem}_{idx}{suffix}"
        idx += 1

    with open(candidate, "wb") as f:
        f.write(data)
    return candidate.relative_to(Path.cwd()).as_posix()


def load_artifact_catalog_df() -> pd.DataFrame:
    root = get_artifact_root_path()
    rows: List[Dict[str, Any]] = []
    for p in root.rglob("*"):
        if not p.is_file():
            continue
        stat = p.stat()
        mtime = datetime.fromtimestamp(stat.st_mtime)
        rel = p.relative_to(Path.cwd()).as_posix()
        name = p.name
        parts = name.split("__", 2)
        stage_key = ""
        try:
            dt = datetime.strptime(parts[0], "%Y%m%d_%H%M%S") if len(parts) >= 1 else mtime
        except Exception:
            dt = mtime
        if len(parts) >= 2:
            stage_key = parts[1]
        rows.append({
            "timestamp": dt.strftime("%Y-%m-%d %H:%M:%S"),
            "year": dt.strftime("%Y"),
            "month": dt.strftime("%m"),
            "day": dt.strftime("%d"),
            "stage_key": stage_key,
            "file_name": _extract_display_name_from_artifact_name(name),
            "file_path": rel,
            "size_kb": round(stat.st_size / 1024, 2),
        })
    if not rows:
        return pd.DataFrame()
    return pd.DataFrame(rows).sort_values("timestamp", ascending=False).reset_index(drop=True)


def get_task_root_path() -> Path:
    root = Path.cwd() / TASK_DIR_NAME
    root.mkdir(parents=True, exist_ok=True)
    return root


def _make_ai_task_id() -> str:
    return f"ai_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"


def _get_ai_task_dir(task_id: str) -> Path:
    return get_task_root_path() / str(task_id)


def _get_ai_task_meta_path(task_id: str) -> Path:
    return _get_ai_task_dir(task_id) / "meta.json"


def _get_ai_task_df_path(task_id: str) -> Path:
    return _get_ai_task_dir(task_id) / "df_work.pkl"


def _get_ai_task_source_df_path(task_id: str) -> Path:
    return _get_ai_task_dir(task_id) / "source_df.pkl"


def init_ai_task_dataframe(df_source: pd.DataFrame, total_rows: int) -> pd.DataFrame:
    df = df_source.reset_index(drop=True).copy()
    total = max(0, min(int(total_rows), len(df)))

    # æ¯æ¬¡æ–°å»ºä»»åŠ¡éƒ½é‡ç½® AI ç»“æœåˆ—ï¼Œé¿å…ä½¿ç”¨æ—§ç»“æœè¯¯åˆ¤ã€‚
    df[COL_AI_EXTRACTED_AMOUNT] = None
    df[COL_AI_MATCH] = None
    df[COL_AI_NOTE] = ""
    df[COL_AI_TASK_STATUS] = AI_ROW_STATUS_PENDING
    df[COL_AI_TASK_UPDATED_AT] = ""

    if total < len(df):
        out_scope_idx = df.index[total:]
        df.loc[out_scope_idx, COL_AI_TASK_STATUS] = AI_ROW_STATUS_OUT_OF_SCOPE
        df.loc[out_scope_idx, COL_AI_NOTE] = "æœªå¤„ç†ï¼ˆè¶…è¿‡æœ¬æ¬¡æœ€å¤§å¤„ç†è¡Œæ•°é™åˆ¶ï¼‰"

    return df


def create_ai_task_state(
    df_source: pd.DataFrame,
    source_file: str,
    col_amount: str,
    col_shot: str,
    total_rows: int,
    model_name: str,
    max_images: int,
    min_interval_sec: float,
    max_retries: int,
    backoff_base_sec: float,
) -> Dict[str, Any]:
    src = df_source.reset_index(drop=True).copy()
    total = max(0, min(int(total_rows), len(src)))
    task_id = _make_ai_task_id()

    task = {
        "task_id": task_id,
        "created_at": now_iso(),
        "updated_at": now_iso(),
        "finished_at": "",
        "status": AI_TASK_STATUS_PAUSED,
        "source_file": source_file,
        "input_rows": len(src),
        "total": total,
        "next_idx": 0,
        "col_amount": col_amount,
        "col_shot": col_shot,
        "model_name": model_name,
        "max_images": int(max_images),
        "min_interval_sec": float(min_interval_sec),
        "max_retries": int(max_retries),
        "backoff_base_sec": float(backoff_base_sec),
        "history_logged": False,
        "alignment_report": None,
        "error_message": "",
        "artifacts": [],
        "df_work": init_ai_task_dataframe(src, total),
        "source_df": src,
    }
    save_ai_task_state(task)
    return task


def save_ai_task_state(task: Dict[str, Any]) -> None:
    task_id = str(task.get("task_id", "")).strip()
    if not task_id:
        return

    task_dir = _get_ai_task_dir(task_id)
    task_dir.mkdir(parents=True, exist_ok=True)

    with _AI_TASK_FILE_LOCK:
        df_work = task.get("df_work")
        if isinstance(df_work, pd.DataFrame):
            df_work.to_pickle(_get_ai_task_df_path(task_id))

        source_df = task.get("source_df")
        src_path = _get_ai_task_source_df_path(task_id)
        if isinstance(source_df, pd.DataFrame) and (not src_path.exists()):
            source_df.to_pickle(src_path)

        meta = {k: v for k, v in task.items() if k not in ("df_work", "source_df")}
        meta["updated_at"] = now_iso()

        meta_path = _get_ai_task_meta_path(task_id)
        tmp_meta_path = task_dir / "meta.tmp.json"
        with open(tmp_meta_path, "w", encoding="utf-8") as f:
            json.dump(_json_safe(meta), f, ensure_ascii=False, indent=2)
        os.replace(tmp_meta_path, meta_path)


def load_ai_task_state(task_id: str) -> Optional[Dict[str, Any]]:
    if not task_id:
        return None

    meta_path = _get_ai_task_meta_path(task_id)
    if not meta_path.exists():
        return None

    with _AI_TASK_FILE_LOCK:
        try:
            with open(meta_path, "r", encoding="utf-8") as f:
                meta = json.load(f)
        except Exception:
            return None

        if not isinstance(meta, dict):
            return None

        df_path = _get_ai_task_df_path(task_id)
        src_path = _get_ai_task_source_df_path(task_id)
        try:
            meta["df_work"] = pd.read_pickle(df_path) if df_path.exists() else pd.DataFrame()
        except Exception:
            meta["df_work"] = pd.DataFrame()
        try:
            meta["source_df"] = pd.read_pickle(src_path) if src_path.exists() else pd.DataFrame()
        except Exception:
            meta["source_df"] = pd.DataFrame()
        return meta


def load_latest_ai_task_state(prefer_active: bool = True) -> Optional[Dict[str, Any]]:
    root = get_task_root_path()
    if not root.exists():
        return None

    candidates: List[Tuple[float, str, str]] = []
    for meta_path in root.glob("*/meta.json"):
        task_id = meta_path.parent.name
        try:
            with open(meta_path, "r", encoding="utf-8") as f:
                meta = json.load(f)
        except Exception:
            continue
        if not isinstance(meta, dict):
            continue

        status = str(meta.get("status", ""))
        if prefer_active and status not in (AI_TASK_STATUS_RUNNING, AI_TASK_STATUS_PAUSED, AI_TASK_STATUS_ERROR):
            continue
        ts = float(meta_path.stat().st_mtime)
        candidates.append((ts, task_id, status))

    if not candidates and prefer_active:
        return load_latest_ai_task_state(prefer_active=False)
    if not candidates:
        return None

    candidates.sort(reverse=True)
    return load_ai_task_state(candidates[0][1])


def split_ai_task_frames(task: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
    df_work = task.get("df_work")
    if not isinstance(df_work, pd.DataFrame) or df_work.empty:
        empty = pd.DataFrame()
        return {"ok": empty, "bad": empty, "pending": empty, "processed": empty}

    total = max(0, min(int(task.get("total", len(df_work))), len(df_work)))
    in_scope = df_work.iloc[:total].copy()
    out_scope = df_work.iloc[total:].copy()

    if COL_AI_TASK_STATUS in in_scope.columns:
        processed_mask = in_scope[COL_AI_TASK_STATUS] == AI_ROW_STATUS_DONE
    else:
        if COL_AI_MATCH in in_scope.columns:
            processed_mask = in_scope[COL_AI_MATCH].notna()
        else:
            processed_mask = pd.Series(False, index=in_scope.index)

    processed = in_scope[processed_mask].copy()
    pending_scope = in_scope[~processed_mask].copy()
    pending = pd.concat([pending_scope, out_scope], axis=0).copy()

    ok = processed[processed[COL_AI_MATCH] == True].copy() if COL_AI_MATCH in processed.columns else pd.DataFrame()
    bad = processed[processed[COL_AI_MATCH] != True].copy() if COL_AI_MATCH in processed.columns else pd.DataFrame()

    return {"ok": ok, "bad": bad, "pending": pending, "processed": processed}


def summarize_ai_task(task: Dict[str, Any]) -> Dict[str, Any]:
    frames = split_ai_task_frames(task)
    total = int(task.get("total", 0))
    processed_rows = len(frames["processed"])
    return {
        "total": total,
        "processed_rows": processed_rows,
        "pending_rows": max(0, total - processed_rows),
        "ok_rows": len(frames["ok"]),
        "bad_rows": len(frames["bad"]),
    }


def task_status_label(status: str) -> str:
    m = {
        AI_TASK_STATUS_RUNNING: "è¿è¡Œä¸­",
        AI_TASK_STATUS_PAUSED: "å·²æš‚åœ",
        AI_TASK_STATUS_COMPLETED: "å·²å®Œæˆ",
        AI_TASK_STATUS_ERROR: "é”™è¯¯",
    }
    return m.get(str(status), str(status))


def _json_safe(value: Any) -> Any:
    if isinstance(value, (str, int, float, bool)) or value is None:
        return value
    if isinstance(value, (list, tuple)):
        return [_json_safe(v) for v in value]
    if isinstance(value, dict):
        return {str(k): _json_safe(v) for k, v in value.items()}
    return str(value)


def append_operation_history(stage: str, action: str, detail: Dict[str, Any]) -> None:
    now = datetime.now()
    record = {
        "timestamp": now.strftime("%Y-%m-%d %H:%M:%S"),
        "year": now.strftime("%Y"),
        "month": now.strftime("%m"),
        "day": now.strftime("%d"),
        "stage": stage,
        "action": action,
        "operator": os.getenv("USERNAME", ""),
    }
    record.update(_json_safe(detail))
    line = json.dumps(record, ensure_ascii=False)

    try:
        with open(get_history_file_path(), "a", encoding="utf-8") as f:
            f.write(line + "\n")
    except Exception as e:
        st.session_state["history_write_error"] = str(e)


def load_operation_history_df() -> pd.DataFrame:
    path = get_history_file_path()
    if not path.exists():
        return pd.DataFrame()

    records: List[Dict[str, Any]] = []
    try:
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                s = line.strip()
                if not s:
                    continue
                try:
                    obj = json.loads(s)
                    if isinstance(obj, dict):
                        records.append(obj)
                except Exception:
                    continue
    except Exception as e:
        st.session_state["history_read_error"] = str(e)
        return pd.DataFrame()

    if not records:
        return pd.DataFrame()
    df_hist = pd.DataFrame(records)
    if "timestamp" in df_hist.columns:
        df_hist = df_hist.sort_values("timestamp", ascending=False).reset_index(drop=True)
    return df_hist


def build_history_download_name(prefix: str = "æ“ä½œå†å²") -> str:
    return f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{prefix}.csv"


_WIDGET_KEY_CALL_COUNTS: Dict[str, int] = {}


def unique_widget_key(base: str) -> str:
    """
    ç”Ÿæˆå½“å‰è„šæœ¬è¿è¡Œå‘¨æœŸå†…çš„å”¯ä¸€ widget keyã€‚
    ç”¨äºéƒ¨ç½²ç¯å¢ƒå‡ºç°ä»£ç å—é‡å¤æ‰§è¡Œæ—¶ï¼Œé¿å… StreamlitDuplicateElementKeyã€‚
    """
    count = _WIDGET_KEY_CALL_COUNTS.get(base, 0) + 1
    _WIDGET_KEY_CALL_COUNTS[base] = count
    return f"{base}__{count}"


def render_preview_dataframe(
    df: pd.DataFrame,
    *,
    title: str,
    key_prefix: str,
    default_rows: int = 50,
    height: int = 380,
    expanded: bool = False,
) -> None:
    """å¯æŠ˜å çš„æ•°æ®é¢„è§ˆï¼šæŒ‰éœ€å±•å¼€ï¼Œå¹¶æ”¯æŒé€‰æ‹©å±•ç¤ºæ¡æ•°ã€‚"""
    total_rows = int(len(df)) if isinstance(df, pd.DataFrame) else 0
    with st.expander(f"{title}ï¼ˆå…± {total_rows} è¡Œï¼‰", expanded=expanded):
        if total_rows <= 0:
            st.info("æš‚æ— æ•°æ®ã€‚")
            return

        row_options: List[Any] = [20, 50, 100, 200, 500, "å…¨éƒ¨"]
        if default_rows not in row_options:
            default_rows = 50

        selected_rows = st.selectbox(
            "å±•ç¤ºæ¡æ•°",
            options=row_options,
            index=row_options.index(default_rows),
            key=f"{key_prefix}_preview_rows",
            format_func=lambda v: f"{v} æ¡" if isinstance(v, int) else str(v),
        )

        if selected_rows == "å…¨éƒ¨":
            df_to_show = df
        else:
            df_to_show = df.head(int(selected_rows))

        st.caption(f"å½“å‰æ˜¾ç¤º {len(df_to_show)} / {total_rows} è¡Œ")
        st.dataframe(df_to_show, use_container_width=True, height=height)


def safe_strip_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ã€å¼ºé˜²å‘è¦æ±‚ã€‘è¯»å–åç«‹åˆ» strip åˆ—å"""
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    return df


def get_uploaded_bytes(uploaded_file) -> bytes:
    """UploadedFile -> bytesï¼ˆé¿å…æ–‡ä»¶æŒ‡é’ˆåå¤è¯»å–é—®é¢˜ï¼‰"""
    if uploaded_file is None:
        return b""
    try:
        return uploaded_file.getvalue()
    except Exception:
        try:
            uploaded_file.seek(0)
            return uploaded_file.read()
        except Exception:
            return b""


def read_table(uploaded_file) -> pd.DataFrame:
    """è¯»å– xlsx/xls/csvï¼Œå¹¶åœ¨ç¬¬ä¸€æ—¶é—´ strip åˆ—å"""
    if uploaded_file is None:
        return pd.DataFrame()

    filename = uploaded_file.name.lower()
    try:
        if filename.endswith((".xlsx", ".xls")):
            uploaded_file.seek(0)
            # å…³é”®å­—æ®µï¼ˆè®¢å•å·/è´¦å·/å•å·ï¼‰éœ€ä¿æŒæ–‡æœ¬ï¼Œé¿å…ç§‘å­¦è®¡æ•°æ³•å’Œç²¾åº¦é£é™©
            df = pd.read_excel(
                uploaded_file,
                engine="openpyxl",
                dtype=str,
                keep_default_na=False,
                na_filter=False
            )  # é»˜è®¤ç¬¬ä¸€ä¸ªsheet
        elif filename.endswith(".csv"):
            uploaded_file.seek(0)
            try:
                df = pd.read_csv(
                    uploaded_file,
                    encoding="utf-8",
                    dtype=str,
                    keep_default_na=False,
                    na_filter=False
                )
            except UnicodeDecodeError:
                uploaded_file.seek(0)
                df = pd.read_csv(
                    uploaded_file,
                    encoding="gbk",
                    dtype=str,
                    keep_default_na=False,
                    na_filter=False
                )
        else:
            raise ValueError("ä»…æ”¯æŒ .xlsx / .xls / .csv")
    except Exception as e:
        raise RuntimeError(f"æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{e}")

    return safe_strip_columns(df)


def find_first_existing_column(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
    cols = set(df.columns)
    for c in candidates:
        if c in cols:
            return c
    return None


def ensure_required_columns(df: pd.DataFrame, required_map: Dict[str, List[str]]) -> Dict[str, str]:
    matched = {}
    missing = []
    cols = set(df.columns)
    for desc, candidates in required_map.items():
        col = None
        for candidate in candidates:
            if candidate in cols:
                col = candidate
                break
        if not col:
            missing.append(f"{desc}ï¼ˆå€™é€‰ï¼š{candidates}ï¼‰")
        else:
            matched[desc] = col
    if missing:
        raise ValueError("ç¼ºå°‘å¿…è¦åˆ—ï¼š\n- " + "\n- ".join(missing))
    return matched


@lru_cache(maxsize=8192)
def _parse_money_text(value_text: str) -> Optional[float]:
    s2 = REGEX_MONEY_CLEAN.sub("", value_text)
    if s2 in ("", ".", "-", "-."):
        return None
    try:
        return float(s2)
    except Exception:
        return None


def parse_money(value: Any) -> Optional[float]:
    if value is None or (isinstance(value, float) and math.isnan(value)):
        return None
    s = str(value).strip()
    if s == "":
        return None
    s = s.replace("ï¿¥", "").replace("Â¥", "").replace("å…ƒ", "").replace(",", "").strip()
    return _parse_money_text(s)


@lru_cache(maxsize=16384)
def _normalize_logistics_text(value: str) -> str:
    return REGEX_NON_ALNUM.sub("", value).strip()


def normalize_logistics_no(raw: Any) -> str:
    if raw is None or (isinstance(raw, float) and math.isnan(raw)):
        return ""
    return _normalize_logistics_text(str(raw).strip())


def validate_row(amount: Any, alipay_account: Any, alipay_name: Any, logistics_no: Any) -> Tuple[bool, str]:
    reasons = []

    money = parse_money(amount)
    if money is None:
        reasons.append("é‡‘é¢å¼‚å¸¸ï¼ˆéæ•°å­—ï¼‰")
    else:
        if money > MAX_REFUND_AMOUNT:
            reasons.append("é‡‘é¢å¼‚å¸¸ï¼ˆé‡‘é¢è¶…æ ‡ï¼‰")

    acct = "" if alipay_account is None else str(alipay_account).strip()
    if acct == "" or (not REGEX_PHONE.match(acct) and not REGEX_EMAIL.match(acct)):
        reasons.append("è´¦å·å¼‚å¸¸ï¼ˆæ”¯ä»˜å®è´¦å·æ ¼å¼ä¸ç¬¦ï¼‰")

    name = "" if alipay_name is None else str(alipay_name).strip()
    if name == "" or not REGEX_CN_NAME.match(name):
        reasons.append("å®åå¼‚å¸¸ï¼ˆéœ€2~5ä¸ªæ±‰å­—ï¼‰")

    lno = normalize_logistics_no(logistics_no)
    if lno == "" or not REGEX_LOGISTICS.match(lno):
        reasons.append("å•å·å¼‚å¸¸ï¼ˆç‰©æµå•å·éœ€10~16ä½å­—æ¯æ•°å­—ä¸”åŒ…å«æ•°å­—ï¼‰")

    if reasons:
        return False, "ï¼›".join(reasons)
    return True, ""


# =============================================================================
# ã€2.5ã€‘å…¥åº“å•å·ä¸Šä¼ ä¸åŒ¹é…
# =============================================================================

def build_inbound_set(df_inbound: pd.DataFrame, logistics_col: str) -> set:
    """æŠŠå…¥åº“è¡¨æŒ‡å®šåˆ—è½¬ä¸ºæ ‡å‡†åŒ–å•å·é›†åˆ"""
    normalized_values = (normalize_logistics_no(v) for v in df_inbound[logistics_col].tolist())
    return {v for v in normalized_values if v}


def attach_inbound_flag(df: pd.DataFrame, logistics_col: str, inbound_set: set) -> pd.DataFrame:
    """åœ¨ Tab1 çš„ df ä¸Šæ ‡è®°æ˜¯å¦å·²å…¥åº“"""
    df = df.copy()

    if COL_INBOUND_FLAG not in df.columns:
        df[COL_INBOUND_FLAG] = ""
    if COL_INBOUND_NOTE not in df.columns:
        df[COL_INBOUND_NOTE] = ""

    if not inbound_set:
        return df

    def _flag(x):
        n = normalize_logistics_no(x)
        return "å·²å…¥åº“" if n in inbound_set else ""

    df[COL_INBOUND_FLAG] = df[logistics_col].apply(_flag)
    df[COL_INBOUND_NOTE] = df[COL_INBOUND_FLAG].apply(lambda v: "åŒ¹é…åˆ°å·²å…¥åº“è¡¨" if v == "å·²å…¥åº“" else "")
    return df


# =============================================================================
# ã€3ã€‘é“¾æ¥è§£æ + Excel è¶…é“¾æ¥æŠ½å–/å†™å›
# =============================================================================

def _dedupe_preserve_order(values: List[str], max_items: Optional[int] = None) -> List[str]:
    seen = set()
    out: List[str] = []
    for v in values:
        item = v.strip()
        if not item or item in seen:
            continue
        out.append(item)
        seen.add(item)
        if max_items is not None and len(out) >= max_items:
            break
    return out


def extract_urls_from_cell(cell_value: Any) -> List[str]:
    """ä»å•å…ƒæ ¼æ–‡æœ¬ä¸­æå– URLï¼ˆæ”¯æŒ markdown/è£¸é“¾ï¼‰"""
    if cell_value is None or (isinstance(cell_value, float) and math.isnan(cell_value)):
        return []
    s = str(cell_value).strip()
    if not s:
        return []
    urls: List[str] = []
    urls.extend(REGEX_URL_IN_PARENS.findall(s))
    urls.extend(REGEX_URL_GENERIC.findall(s))
    # å»é‡ä¿æŒé¡ºåº
    return _dedupe_preserve_order(urls)


@lru_cache(maxsize=4096)
def _normalize_preview_url_cached(url: str) -> Tuple[str, ...]:
    if not url:
        return tuple()
    try:
        parsed = urlparse(url)
        qs = parse_qs(parsed.query)
        if "url" in qs:
            raw = unquote(qs["url"][0])
            parts = REGEX_PREVIEW_SPLIT.split(raw)
            extracted = [p.strip() for p in parts if p.strip().startswith("http")]
            if extracted:
                return tuple(extracted)
    except Exception:
        pass
    return (url,)


def normalize_preview_url(url: str) -> List[str]:
    """
    å…¼å®¹ exportFilePreview?url=... å½¢å¼ï¼ŒæŠŠçœŸå®é“¾æ¥æ‹†å‡ºæ¥
    ç¤ºä¾‹ï¼š
    https://work.bytenew.com/app.html#/exportFilePreview?url=https%3A%2F%2F...jpeg__https%3A%2F%2F...jpeg
    """
    if not url:
        return []
    return list(_normalize_preview_url_cached(str(url).strip()))


def pick_first_image_url(urls: List[str]) -> Optional[str]:
    """ä¼˜å…ˆæŒ‘å›¾ç‰‡é“¾æ¥ï¼Œå¦åˆ™è¿”å›ç¬¬ä¸€ä¸ª http(s)"""
    if not urls:
        return None
    expanded = []
    for u in urls:
        expanded.extend(normalize_preview_url(u))
    for u in expanded:
        if u.lower().endswith(IMAGE_EXTENSIONS):
            return u
    for u in expanded:
        if u.startswith("http"):
            return u
    return None


def pick_image_urls(urls: List[str], max_images: int = 4) -> List[str]:
    """
    âœ… å¤šå›¾ï¼šä»å•å…ƒæ ¼é‡Œè§£æå‡ºçš„é“¾æ¥ï¼Œæ‹†å‡ºé¢„è§ˆé‡Œçš„å¤šå›¾ï¼ˆ__ åˆ†éš”ï¼‰ï¼Œå–å‰ max_images å¼ 
    """
    if not urls:
        return []
    expanded: List[str] = []
    for u in urls:
        expanded.extend(normalize_preview_url(u))

    # åªä¿ç•™å›¾ç‰‡ç›´é“¾
    imgs = [u for u in expanded if u.lower().endswith(IMAGE_EXTENSIONS)]

    # å»é‡ä¿æŒé¡ºåº
    out = _dedupe_preserve_order(imgs, max_items=max_images)

    # å…œåº•ï¼šæ²¡æœ‰å›¾ç‰‡åç¼€æ—¶ä¹Ÿä¿ç•™ httpï¼ˆæœ‰äº›é“¾æ¥å¯èƒ½æ²¡åç¼€ä½†å¯è®¿é—®ï¼‰
    if not out:
        http_urls = [u for u in expanded if u.startswith("http")]
        out = _dedupe_preserve_order(http_urls, max_items=max_images)

    return out


@lru_cache(maxsize=8192)
def _extract_image_urls_from_text(raw_text: str, max_images: int) -> Tuple[str, ...]:
    urls = extract_urls_from_cell(raw_text)
    img_urls = pick_image_urls(urls, max_images=max_images)
    if img_urls:
        return tuple(img_urls)

    if raw_text.startswith("http"):
        expanded = normalize_preview_url(raw_text)
        image_candidates = [u for u in expanded if u.lower().endswith(IMAGE_EXTENSIONS)]
        if image_candidates:
            return tuple(image_candidates[:max_images])
        return tuple(expanded[:max_images])
    return tuple()


def extract_image_urls_from_cell_value(cell_value: Any, max_images: int = 4) -> List[str]:
    if cell_value is None or (isinstance(cell_value, float) and math.isnan(cell_value)):
        return []
    raw_text = str(cell_value).strip()
    if not raw_text:
        return []
    return list(_extract_image_urls_from_text(raw_text, int(max_images)))


def extract_hyperlinks_from_excel(file_bytes: bytes, target_header: str, n_rows: Optional[int] = None) -> List[Optional[str]]:
    """
    ä» Excel ä¸­æå–æŒ‡å®šåˆ—æ¯è¡Œçš„é“¾æ¥ URLï¼Œå…¼å®¹ï¼š
    1) cell.hyperlink.targetï¼ˆåŸç”Ÿè¶…é“¾æ¥ï¼‰
    2) =HYPERLINK("url","é¢„è§ˆ") / =HYPERLINK("url";"é¢„è§ˆ") å…¬å¼
    3) tooltip / comment é‡Œè— URLï¼ˆå¾ˆå¤šå¯¼å‡ºâ€œé¢„è§ˆâ€å°±æ˜¯è¿™ç§ï¼‰
    å¹¶ä¸”æŒ‰ n_rows å¯¹é½ pandas çš„æ•°æ®è¡Œæ•°ï¼ˆé¿å… ws.max_row å¯¼è‡´é”™ä½ï¼‰
    """
    if not file_bytes:
        return []
    try:
        # âœ… å…³é”®ï¼šdata_only=False æ‰èƒ½æ‹¿åˆ°å…¬å¼æœ¬ä½“
        wb = load_workbook(BytesIO(file_bytes), data_only=False)
        # âœ… ä¸ pandas é»˜è®¤ sheet å¯¹é½ï¼šç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
        ws = wb.worksheets[0]

        # è¯»å–è¡¨å¤´ï¼ˆç¬¬1è¡Œï¼‰
        headers = []
        for c in range(1, ws.max_column + 1):
            v = ws.cell(row=1, column=c).value
            headers.append(str(v).strip() if v is not None else "")

        if target_header not in headers:
            return []

        col_idx = headers.index(target_header) + 1

        # å…¬å¼è¶…é“¾æ¥ï¼šæ”¯æŒ , æˆ– ; åˆ†éš”
        re_hyper = REGEX_EXCEL_HYPERLINK_FORMULA
        # URL å…œåº•æŠ½å–
        re_url = REGEX_EXCEL_URL_FALLBACK

        # âœ… è¡Œæ•°å¯¹é½ï¼šåªå– pandas å®é™…è¡Œæ•°
        if n_rows is None:
            end_row = ws.max_row
        else:
            end_row = 1 + int(n_rows)  # è¡¨å¤´1è¡Œ + n_rowsæ•°æ®è¡Œ

        links: List[Optional[str]] = []
        for r in range(2, end_row + 1):
            cell = ws.cell(row=r, column=col_idx)
            url = None

            # 1) åŸç”Ÿ hyperlink
            if cell.hyperlink and getattr(cell.hyperlink, "target", None):
                url = str(cell.hyperlink.target).strip()

            # 2) HYPERLINKå…¬å¼
            if not url:
                v = cell.value
                if isinstance(v, str):
                    m = re_hyper.search(v)
                    if m:
                        url = m.group(1).strip()

            # 3) tooltip / comment å…œåº•ï¼ˆé»„è‰²æç¤ºæ¡†å¸¸åœ¨è¿™é‡Œï¼‰
            if not url:
                tip = None
                try:
                    tip = getattr(cell.hyperlink, "tooltip", None) if cell.hyperlink else None
                except Exception:
                    tip = None
                if isinstance(tip, str):
                    m = re_url.search(tip)
                    if m:
                        url = m.group(1).strip()

            if not url and cell.comment and isinstance(cell.comment.text, str):
                m = re_url.search(cell.comment.text)
                if m:
                    url = m.group(1).strip()

            links.append(url if url else None)

        return links
    except Exception:
        return []


def attach_hyperlink_helper_column(df: pd.DataFrame, file_bytes: bytes, screenshot_col: str) -> pd.DataFrame:
    """
    è‹¥ Excel æˆªå›¾åˆ—æ˜¯â€œè¶…é“¾æ¥/å…¬å¼è¶…é“¾æ¥/tooltip/æ‰¹æ³¨â€ï¼Œpandas è¯»ä¸åˆ° targetï¼Œ
    ç”¨ openpyxl æŠ½å‡º URL å­˜å…¥è¾…åŠ©åˆ— {screenshot_col}__hyperlink
    âœ… å¼ºåˆ¶æŒ‰ df è¡Œæ•°å¯¹é½ï¼Œä¸å†è¦æ±‚ len(links)==len(df)
    """
    df = df.copy()
    if not file_bytes or df.empty:
        return df

    links = extract_hyperlinks_from_excel(file_bytes, screenshot_col, n_rows=len(df))

    # å¯¹é½ï¼šä¸è¶³è¡¥ Noneï¼Œè¶…å‡ºæˆªæ–­
    if len(links) < len(df):
        links = links + [None] * (len(df) - len(links))
    elif len(links) > len(df):
        links = links[:len(df)]

    df[screenshot_col + HYPERLINK_SUFFIX] = links
    return df


def _is_identifier_column(col_name: str) -> bool:
    name = str(col_name)
    return any(k in name for k in IDENTIFIER_COLUMN_KEYWORDS)


def _normalize_scientific_text(s: str) -> str:
    v = s.strip()
    if not v or not REGEX_SCI_NUMBER.match(v):
        return v
    try:
        d = Decimal(v)
    except InvalidOperation:
        return v
    if d == d.to_integral_value():
        return format(d.quantize(Decimal("1")), "f")
    out = format(d, "f")
    out = out.rstrip("0").rstrip(".")
    return out if out else "0"


def _normalize_identifier_cell(value: Any) -> str:
    if value is None:
        return ""
    if isinstance(value, float):
        if math.isnan(value):
            return ""
        if value.is_integer():
            return format(value, ".0f")
        out = format(value, "f").rstrip("0").rstrip(".")
        return out if out else "0"
    if isinstance(value, int):
        return str(value)
    s = str(value).strip()
    if not s:
        return ""
    return _normalize_scientific_text(s)


def find_column_with_fallback(
    df: pd.DataFrame,
    candidates: List[str],
    fuzzy_keywords: Optional[List[str]] = None
) -> Optional[str]:
    col = find_first_existing_column(df, candidates)
    if col:
        return col
    if not fuzzy_keywords:
        return None

    for c in df.columns:
        name = str(c).strip()
        lname = name.lower()
        if any(k.lower() in lname for k in fuzzy_keywords):
            return c
    return None


def build_row_identity_keys(df: pd.DataFrame, id_col: str, order_col: str, logistics_col: str) -> Tuple[List[Tuple[str, str, str]], List[str]]:
    keys: List[Tuple[str, str, str]] = []
    logistics_keys: List[str] = []

    id_values = df[id_col].tolist()
    order_values = df[order_col].tolist()
    logistics_values = df[logistics_col].tolist()

    for id_v, order_v, logistics_v in zip(id_values, order_values, logistics_values):
        id_key = _normalize_identifier_cell(id_v)
        order_key = _normalize_identifier_cell(order_v)
        logistics_key = normalize_logistics_no(logistics_v)
        keys.append((id_key, order_key, logistics_key))
        if logistics_key:
            logistics_keys.append(logistics_key)

    return keys, logistics_keys


def compare_source_and_processed(
    source_df: pd.DataFrame,
    processed_df: pd.DataFrame,
    stage_name: str
) -> Dict[str, Any]:
    report: Dict[str, Any] = {
        "stage": stage_name,
        "can_compare": False,
        "ok": False,
        "source_rows": len(source_df),
        "processed_rows": len(processed_df),
    }

    if source_df is None or processed_df is None or source_df.empty or processed_df.empty:
        report["message"] = "æºæ•°æ®æˆ–å¤„ç†åæ•°æ®ä¸ºç©ºï¼Œæ— æ³•æ ¡éªŒã€‚"
        return report

    src_id = find_column_with_fallback(source_df, COL_ID_CANDIDATES, fuzzy_keywords=["id", "æ—ºæ—º"])
    src_order = find_column_with_fallback(source_df, COL_ORDER_NO_CANDIDATES, fuzzy_keywords=["è®¢å•"])
    src_lno = find_column_with_fallback(source_df, COL_LOGISTICS_NO_CANDIDATES, fuzzy_keywords=["ç‰©æµ", "å¿«é€’"])

    dst_id = find_column_with_fallback(processed_df, COL_ID_CANDIDATES, fuzzy_keywords=["id", "æ—ºæ—º"])
    dst_order = find_column_with_fallback(processed_df, COL_ORDER_NO_CANDIDATES, fuzzy_keywords=["è®¢å•"])
    dst_lno = find_column_with_fallback(processed_df, COL_LOGISTICS_NO_CANDIDATES, fuzzy_keywords=["ç‰©æµ", "å¿«é€’"])

    missing = []
    if not src_id or not dst_id:
        missing.append("ID")
    if not src_order or not dst_order:
        missing.append("è®¢å•å·")
    if not src_lno or not dst_lno:
        missing.append("ç‰©æµå•å·")
    if missing:
        report["message"] = "ç¼ºå°‘å¯¹æ¯”å­—æ®µï¼š" + "ã€".join(missing)
        report["mapping"] = {
            "source_id_col": src_id or "",
            "source_order_col": src_order or "",
            "source_logistics_col": src_lno or "",
            "processed_id_col": dst_id or "",
            "processed_order_col": dst_order or "",
            "processed_logistics_col": dst_lno or "",
        }
        return report

    src_keys, src_logistics = build_row_identity_keys(source_df, src_id, src_order, src_lno)
    dst_keys, dst_logistics = build_row_identity_keys(processed_df, dst_id, dst_order, dst_lno)

    src_counter = Counter(src_keys)
    dst_counter = Counter(dst_keys)
    missing_counter = src_counter - dst_counter
    extra_counter = dst_counter - src_counter

    missing_rows = int(sum(missing_counter.values()))
    extra_rows = int(sum(extra_counter.values()))

    src_logistics_counter = Counter(src_logistics)
    dst_logistics_counter = Counter(dst_logistics)
    src_dup_logistics = int(sum(1 for c in src_logistics_counter.values() if c > 1))
    dst_dup_logistics = int(sum(1 for c in dst_logistics_counter.values() if c > 1))

    diff_preview: List[Dict[str, Any]] = []
    for key, cnt in missing_counter.items():
        diff_preview.append({"å·®å¼‚ç±»å‹": "æºæœ‰ä½†å¤„ç†åæ— ", "ID": key[0], "è®¢å•å·": key[1], "ç‰©æµå•å·": key[2], "æ•°é‡": cnt})
        if len(diff_preview) >= 20:
            break
    if len(diff_preview) < 20:
        for key, cnt in extra_counter.items():
            diff_preview.append({"å·®å¼‚ç±»å‹": "å¤„ç†åæœ‰ä½†æºæ— ", "ID": key[0], "è®¢å•å·": key[1], "ç‰©æµå•å·": key[2], "æ•°é‡": cnt})
            if len(diff_preview) >= 20:
                break

    report.update({
        "can_compare": True,
        "ok": (missing_rows == 0 and extra_rows == 0),
        "missing_rows": missing_rows,
        "extra_rows": extra_rows,
        "source_duplicate_logistics": src_dup_logistics,
        "processed_duplicate_logistics": dst_dup_logistics,
        "mapping": {
            "source_id_col": src_id,
            "source_order_col": src_order,
            "source_logistics_col": src_lno,
            "processed_id_col": dst_id,
            "processed_order_col": dst_order,
            "processed_logistics_col": dst_lno,
        },
        "diff_preview": diff_preview,
    })
    return report


def render_alignment_report(report: Dict[str, Any], title: str) -> None:
    st.markdown(f"#### ğŸ” {title}")

    if not report.get("can_compare"):
        st.warning(f"âš ï¸ æœªå®Œæˆä¸€è‡´æ€§æ ¡éªŒï¼š{report.get('message', 'ç¼ºå°‘å¿…è¦å­—æ®µã€‚')}")
        mapping = report.get("mapping", {})
        if mapping:
            st.caption(
                "å­—æ®µè¯†åˆ«æƒ…å†µï¼š"
                f"æº[ID:{mapping.get('source_id_col', 'æ— ')} / è®¢å•:{mapping.get('source_order_col', 'æ— ')} / ç‰©æµ:{mapping.get('source_logistics_col', 'æ— ')}]ï¼›"
                f"ç»“æœ[ID:{mapping.get('processed_id_col', 'æ— ')} / è®¢å•:{mapping.get('processed_order_col', 'æ— ')} / ç‰©æµ:{mapping.get('processed_logistics_col', 'æ— ')}]"
            )
        return

    if report.get("ok"):
        st.success("âœ… ä¸€è‡´æ€§æ ¡éªŒé€šè¿‡ï¼šå¤„ç†åæ•°æ®ä¸æºæ•°æ®æŒ‰ ID/è®¢å•å·/ç‰©æµå•å· å®Œæ•´å¯¹åº”ã€‚")
    else:
        st.error("âŒ ä¸€è‡´æ€§æ ¡éªŒæœªé€šè¿‡ï¼šå‘ç°æºæ•°æ®ä¸å¤„ç†åæ•°æ®å­˜åœ¨ä¸ä¸€è‡´ã€‚è¯·å…ˆç¡®è®¤åå†ç»§ç»­ã€‚")

    c1, c2, c3, c4 = st.columns(4)
    c1.metric("æºæ•°æ®è¡Œæ•°", int(report.get("source_rows", 0)))
    c2.metric("å¤„ç†åè¡Œæ•°", int(report.get("processed_rows", 0)))
    c3.metric("æºæœ‰ç»“æœæ— ", int(report.get("missing_rows", 0)))
    c4.metric("ç»“æœæœ‰æºæ— ", int(report.get("extra_rows", 0)))

    mapping = report.get("mapping", {})
    st.caption(
        "å­—æ®µæ˜ å°„ï¼š"
        f"æº[ID:{mapping.get('source_id_col', 'æ— ')} / è®¢å•:{mapping.get('source_order_col', 'æ— ')} / ç‰©æµ:{mapping.get('source_logistics_col', 'æ— ')}]ï¼›"
        f"ç»“æœ[ID:{mapping.get('processed_id_col', 'æ— ')} / è®¢å•:{mapping.get('processed_order_col', 'æ— ')} / ç‰©æµ:{mapping.get('processed_logistics_col', 'æ— ')}]"
    )

    src_dup = int(report.get("source_duplicate_logistics", 0))
    dst_dup = int(report.get("processed_duplicate_logistics", 0))
    if src_dup > 0 or dst_dup > 0:
        st.warning(f"âš ï¸ ç‰©æµå•å·é‡å¤æ£€æŸ¥ï¼šæºæ•°æ®é‡å¤ {src_dup} ä¸ªï¼Œå¤„ç†åé‡å¤ {dst_dup} ä¸ªã€‚")

    diff_preview = report.get("diff_preview", [])
    if diff_preview:
        with st.expander("æŸ¥çœ‹å·®å¼‚æ ·ä¾‹ï¼ˆæœ€å¤š20æ¡ï¼‰", expanded=False):
            st.dataframe(pd.DataFrame(diff_preview), use_container_width=True, height=260)

def df_to_excel_bytes(
    df: pd.DataFrame,
    sheet_name: str = "sheet1",
    hyperlink_cols: Optional[List[str]] = None
) -> bytes:
    """
    DataFrame -> Excel bytesï¼Œå¹¶ä¸ºæŒ‡å®šåˆ—å†™å›è¶…é“¾æ¥ï¼š
    - ä¿ç•™åŸæ–‡å­—ï¼ˆä¾‹å¦‚â€œé¢„è§ˆ/æµè§ˆâ€/é•¿ä¸²æ–‡æœ¬ï¼‰ï¼Œä½†æ•´æ ¼å¯ç‚¹å‡»
    - ä¼˜å…ˆä½¿ç”¨è¾…åŠ©åˆ— {col}__hyperlink çš„ target
    - è‹¥æ— è¾…åŠ©åˆ—ï¼Œåˆ™å°è¯•ä» cell.value æ–‡æœ¬è§£æ URLï¼ˆåŒ…å«é¢„è§ˆé“¾æ¥æ—¶ä¹Ÿèƒ½æ‹† __ï¼‰
    """
    df_export = df.copy()
    identifier_cols = [c for c in df_export.columns if _is_identifier_column(c)]
    for col in identifier_cols:
        df_export[col] = df_export[col].map(_normalize_identifier_cell)

    # è®°å½•æ¯ä¸ªè¶…é“¾æ¥åˆ—å¯¹åº”çš„ target åˆ—è¡¨ï¼ˆæŒ‰è¡Œå¯¹é½ï¼‰
    link_targets: Dict[str, List[Optional[str]]] = {}

    if hyperlink_cols:
        for col in hyperlink_cols:
            helper_col = col + HYPERLINK_SUFFIX
            if helper_col in df_export.columns:
                link_targets[col] = df_export[helper_col].tolist()
                # å¯¼å‡ºè¡¨é‡Œä¸å¸¦è¾…åŠ©åˆ—
                df_export.drop(columns=[helper_col], inplace=True, errors="ignore")

    # å…ˆå†™ Excel
    bio = BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as writer:
        df_export.to_excel(writer, index=False, sheet_name=sheet_name)

    if not hyperlink_cols:
        return bio.getvalue()

    # å†ç”¨ openpyxl å†™å› hyperlink
    bio.seek(0)
    wb = load_workbook(bio)
    ws = wb[sheet_name] if sheet_name in wb.sheetnames else wb.active

    # è¡¨å¤´æ˜ å°„ï¼šåˆ—å->åˆ—å·
    header_map = {}
    for c in range(1, ws.max_column + 1):
        v = ws.cell(row=1, column=c).value
        if v is not None:
            header_map[str(v).strip()] = c

    # å…³é”®æ ‡è¯†å­—æ®µå¼ºåˆ¶æ–‡æœ¬æ ¼å¼ï¼Œé¿å… Excel ç§‘å­¦è®¡æ•°æ³•æ˜¾ç¤º
    for col in identifier_cols:
        if col not in header_map:
            continue
        cidx = header_map[col]
        for r in range(2, ws.max_row + 1):
            cell = ws.cell(row=r, column=cidx)
            if cell.value is None:
                continue
            cell.value = str(cell.value)
            cell.number_format = "@"

    for col in hyperlink_cols:
        if col not in header_map:
            continue
        cidx = header_map[col]
        targets = link_targets.get(col, [])

        # Excel æ•°æ®è¡Œä»ç¬¬2è¡Œå¼€å§‹ï¼ŒDataFrame è¡Œä» 0 å¼€å§‹
        for r in range(2, ws.max_row + 1):
            df_idx = r - 2
            cell = ws.cell(row=r, column=cidx)

            # 1) ä¼˜å…ˆå–è¾…åŠ©åˆ— targetï¼ˆåŸ Excel è¶…é“¾æ¥/å…¬å¼è§£æ/tooltipè§£æå‡ºæ¥çš„URLï¼‰
            target = None
            if targets and df_idx < len(targets):
                target = targets[df_idx]

            # 2) è‹¥æ²¡æœ‰ targetï¼Œåˆ™å°è¯•ä» cell.value æ–‡æœ¬è§£æï¼ˆé€‚é…é•¿ä¸² url æ–‡æœ¬/é¢„è§ˆé“¾æ¥ï¼‰
            if not target:
                val = "" if cell.value is None else str(cell.value).strip()
                urls = extract_urls_from_cell(val)
                # è¿™é‡Œä½¿ç”¨å¤šå›¾é€»è¾‘ï¼Œå–ç¬¬ä¸€å¼ å½“ hyperlinkï¼ˆExcel å•æ ¼åªèƒ½æŒ‚ä¸€ä¸ªï¼‰
                imgs = pick_image_urls(urls, max_images=1)
                target = imgs[0] if imgs else None
                if not target and val.startswith("http"):
                    # å¦‚æœæ˜¯é¢„è§ˆé“¾æ¥ï¼Œå°è¯•æ‹†
                    expanded = normalize_preview_url(val)
                    target = expanded[0] if expanded else val

            if target and str(target).startswith("http"):
                cell.hyperlink = str(target).strip()
                cell.style = "Hyperlink"

    out = BytesIO()
    wb.save(out)
    return out.getvalue()


# =============================================================================
# ã€4ã€‘DashScope è°ƒç”¨ï¼ˆTab2ï¼‰+ å¤šå›¾æç¤ºè¯ + é€Ÿç‡é™åˆ¶ä¸é‡è¯•
# =============================================================================

def get_dashscope_api_key() -> str:
    key = ""
    try:
        key = st.secrets.get("DASHSCOPE_API_KEY", "")
    except Exception:
        key = ""
    if not key:
        key = os.getenv("DASHSCOPE_API_KEY", "")
    return key


def make_vl_prompt(expected_amount: float) -> str:
    """
    âœ… ä¼˜åŒ–æç¤ºè¯ï¼šå¤šå›¾åœºæ™¯ä¸‹ä»æ‰€æœ‰å›¾ç‰‡ä¸­æ‰¾â€œè¿è´¹/å¿«é€’è´¹/é…é€è´¹/é‚®è´¹â€å¯¹åº”é‡‘é¢ï¼Œé¿å…æŠŠå•†å“é‡‘é¢å½“è¿è´¹
    """
    return f"""
ä½ æ˜¯ç”µå•†å”®åè´¢åŠ¡å®¡æ ¸åŠ©æ‰‹ã€‚ç”¨æˆ·å¯èƒ½ä¸Šä¼ äº†å¤šå¼ æˆªå›¾ï¼ˆåŒä¸€æ¡å”®åè®°å½•çš„å›¾ç‰‡ä»ä¸Šåˆ°ä¸‹ä¾æ¬¡æ’åˆ—ï¼‰ã€‚
ä½ çš„ä»»åŠ¡æ˜¯åœ¨æ‰€æœ‰å›¾ç‰‡ä¸­å¯»æ‰¾â€œå¯„å›è¿è´¹/å¿«é€’è´¹/é…é€è´¹/é‚®è´¹/å¯„ä»¶è´¹ç”¨/å®ä»˜è¿è´¹/æ€»è¿è´¹â€ç­‰å­—æ®µå¯¹åº”çš„é‡‘é¢ï¼ˆå•ä½ï¼šå…ƒï¼‰ï¼Œå¹¶ä¸ç”¨æˆ·å¡«å†™é‡‘é¢è¿›è¡Œæ ¸å¯¹ã€‚

ç”¨æˆ·å¡«å†™çš„é€€å›è¿è´¹é‡‘é¢ expected_amount = {expected_amount:.2f} å…ƒã€‚

ã€é‡è¦è§„åˆ™ï¼ˆé¿å…è¯¯åˆ¤ï¼‰ã€‘
1) åªæŠŠä¸â€œè¿è´¹/å¿«é€’è´¹/é…é€è´¹/é‚®è´¹/å¯„ä»¶è´¹ç”¨/è¿è´¹é‡‘é¢/å®ä»˜è¿è´¹/æ€»è¿è´¹â€æ˜ç¡®ç›¸å…³çš„é‡‘é¢å½“ä½œè¿è´¹ã€‚
2) å¦‚æœå›¾ç‰‡é‡Œå‡ºç°â€œå•†å“é‡‘é¢/è®¢å•é‡‘é¢/åˆè®¡/å®ä»˜/ä¼˜æƒ /é€€æ¬¾é‡‘é¢/æ”¯ä»˜é‡‘é¢â€ç­‰å¤šä¸ªé‡‘é¢ï¼š
   - ä¼˜å…ˆé€‰æ‹©ç´§é‚»â€œè¿è´¹/å¿«é€’è´¹/é…é€è´¹/é‚®è´¹/å¯„ä»¶è´¹ç”¨â€æ–‡å­—çš„é‡‘é¢ã€‚
   - ä¸è¦æŠŠå•†å“é‡‘é¢å½“è¿è´¹ã€‚
3) è¿è´¹å¯èƒ½æ˜¾ç¤ºä¸º 0ã€0.00ã€Â¥0ã€å…è¿è´¹ï¼Œä¹Ÿè¦è¯†åˆ«ä¸º 0ã€‚
4) è‹¥æ‰€æœ‰å›¾ç‰‡éƒ½æ²¡æœ‰æ˜ç¡®çš„â€œè¿è´¹/å¿«é€’è´¹/é…é€è´¹/é‚®è´¹/å¯„ä»¶è´¹ç”¨â€å­—æ®µæˆ–å¯¹åº”é‡‘é¢ï¼Œè¯·è¿”å› paid_amount = nullï¼Œå¹¶åœ¨ reason å†™æ¸…æ¥šï¼š
   - â€œå¤šå›¾ä¸­æœªæ‰¾åˆ°è¿è´¹å­—æ®µâ€
   - æˆ– â€œå›¾ç‰‡ä¸ºå•†å“ç‘•ç–µ/èŠå¤©/è®¢å•é¡µï¼Œéè¿è´¹æˆªå›¾â€
   - æˆ– â€œå›¾ç‰‡æ¨¡ç³Š/é®æŒ¡æ— æ³•è¯†åˆ«è¿è´¹é‡‘é¢â€
5) é‡‘é¢æ¯”å¯¹ï¼šå…è®¸è¯¯å·® 0.01ã€‚ç›¸ç­‰åˆ™ is_match=trueï¼Œå¦åˆ™ falseï¼›è¯†åˆ«ä¸åˆ°åˆ™ is_match=nullã€‚
6) ç¦æ­¢çŒœæµ‹ï¼šçœ‹ä¸æ¸…/ä¸ç¡®å®šå°±è¿”å› nullã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
åªè¾“å‡º JSONï¼ˆä¸è¦ä»»ä½•é¢å¤–æ–‡å­—ï¼‰ï¼Œå­—æ®µå¿…é¡»åŒ…å«ï¼š
- paid_amount: æ•°å­—æˆ– null
- is_match: true/false æˆ– null
- reason: ç®€çŸ­æ˜ç¡®è¯´æ˜ï¼ˆå¦‚ï¼šä¸€è‡´/ä¸ä¸€è‡´/å¤šå›¾æœªæ‰¾åˆ°è¿è´¹å­—æ®µ/å›¾ç‰‡æ¨¡ç³Šæ— æ³•è¯†åˆ«/éè¿è´¹æˆªå›¾ç­‰ï¼‰
å¯é€‰å­—æ®µï¼ˆè‹¥èƒ½æä¾›æ›´å¥½ï¼‰ï¼š
- image_index: æ‰¾åˆ°è¿è´¹çš„å›¾ç‰‡åºå·ï¼ˆä»1å¼€å§‹ï¼›æœªæ‰¾åˆ°åˆ™ nullï¼‰
- evidence_text: æ”¯æ’‘åˆ¤æ–­çš„å…³é”®è¯ç‰‡æ®µï¼ˆä¾‹å¦‚â€œè¿è´¹ Â¥8.00â€ï¼‰
- confidence: 0~1 çš„ç½®ä¿¡åº¦ï¼ˆæ— æ³•åˆ¤æ–­åˆ™ 0ï¼‰
""".strip()


def _looks_like_rate_limited(text: str) -> bool:
    if not text:
        return False
    t = str(text).lower()
    keywords = ["rate", "limit", "throttle", "too many", "busy", "ç¹å¿™", "é™æµ", "é¢‘ç‡", "qps", "quota", "exceeded", "429"]
    return any(k in t for k in keywords)


def _sleep_with_ui(seconds: float, ui_slot=None):
    if seconds <= 0:
        return
    if ui_slot is not None:
        ui_slot.info(f"â³ è§¦å‘é€Ÿç‡é™åˆ¶/é€€é¿ç­‰å¾… {seconds:.2f}s â€¦")
    time.sleep(seconds)


def _parse_vl_json(raw_text: str, expected_amount: float) -> Dict[str, Any]:
    """è§£ææ¨¡å‹è¾“å‡ºä¸º dictï¼Œå¹¶åšé‡‘é¢/ä¸€è‡´æ€§å…œåº•"""
    data = None
    try:
        data = json.loads(raw_text)
    except Exception:
        m = re.search(r"\{[\s\S]*\}", raw_text)
        if m:
            try:
                data = json.loads(m.group(0))
            except Exception:
                data = None

    if not isinstance(data, dict):
        return {"paid_amount": None, "is_match": None, "reason": "è¾“å‡ºéJSON", "raw_text": raw_text}

    paid_f = parse_money(data.get("paid_amount")) if data.get("paid_amount") is not None else None
    is_match = data.get("is_match", None)
    if is_match is None and paid_f is not None:
        is_match = (abs(paid_f - expected_amount) <= 0.01)

    reason = str(data.get("reason", "")).strip()
    if not reason:
        reason = "ä¸€è‡´" if is_match else ("å›¾ç‰‡æ¨¡ç³Šæˆ–æ— æ³•è¯†åˆ«é‡‘é¢" if paid_f is None else "ä¸ä¸€è‡´")

    # æŠŠå¯é€‰å­—æ®µé€ä¼ ï¼ˆä¸å½±å“ä½ ç°æœ‰è¡¨ç»“æ„ï¼‰
    out = {
        "paid_amount": paid_f,
        "is_match": is_match,
        "reason": reason,
        "raw_text": raw_text
    }
    for k in ["image_index", "evidence_text", "confidence", "candidates"]:
        if k in data:
            out[k] = data.get(k)
    return out


def call_qwen_vl_extract_amount_multi(image_urls: List[str], expected_amount: float, api_key: str, model: str) -> Dict[str, Any]:
    """
    âœ… å¤šå›¾è°ƒç”¨ï¼šæŠŠåŒä¸€æ¡è®°å½•çš„å¤šå¼ å›¾ç‰‡ä¸€èµ·å‘ç»™æ¨¡å‹ï¼Œè®©æ¨¡å‹åœ¨å¤šå›¾ä¸­æ‰¾è¿è´¹é‡‘é¢
    """
    if dashscope is None:
        return {"paid_amount": None, "is_match": None, "reason": "DashScope SDK æœªå®‰è£…ï¼ˆpip install dashscopeï¼‰", "raw_text": ""}
    if not api_key:
        return {"paid_amount": None, "is_match": None, "reason": "ç¼ºå°‘ DASHSCOPE_API_KEY", "raw_text": ""}
    if not image_urls:
        return {"paid_amount": None, "is_match": None, "reason": "å›¾ç‰‡URLä¸ºç©º", "raw_text": ""}

    prompt = make_vl_prompt(expected_amount)

    # å¤šå›¾ï¼šå…ˆæ”¾å›¾ç‰‡ï¼Œå†æ”¾æ–‡æœ¬ prompt
    content = [{"image": u} for u in image_urls]
    content.append({"text": prompt})
    messages = [{"role": "user", "content": content}]

    try:
        resp = dashscope.MultiModalConversation.call(api_key=api_key, model=model, messages=messages)

        if hasattr(resp, "status_code") and HTTPStatus is not None and resp.status_code != HTTPStatus.OK:
            msg = f"APIå¤±è´¥ï¼š{getattr(resp, 'code', '')} {getattr(resp, 'message', '')}".strip()
            return {
                "paid_amount": None,
                "is_match": None,
                "reason": msg,
                "raw_text": str(resp),
                "status_code": getattr(resp, "status_code", None)
            }

        raw_text = ""
        try:
            raw_text = resp.output.choices[0]["message"]["content"][0]["text"]
        except Exception:
            raw_text = str(resp)

        return _parse_vl_json(raw_text, expected_amount)

    except Exception as e:
        return {"paid_amount": None, "is_match": None, "reason": f"å¼‚å¸¸ï¼š{e}", "raw_text": traceback.format_exc()}


def call_qwen_vl_extract_amount_multi_with_rl(
    image_urls: List[str],
    expected_amount: float,
    api_key: str,
    model: str,
    min_interval_sec: float = 0.8,
    max_retries: int = 4,
    backoff_base_sec: float = 1.0,
    ui_slot=None,
) -> Dict[str, Any]:
    """
    âœ… å¤šå›¾ç‰ˆæœ¬ï¼šé€Ÿç‡é™åˆ¶ + è‡ªåŠ¨é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
    """
    if "ai_last_call_ts" not in st.session_state:
        st.session_state["ai_last_call_ts"] = 0.0

    attempt = 0
    while True:
        # èŠ‚æµï¼šç¡®ä¿è¯·æ±‚é—´éš”
        now = time.monotonic()
        last = float(st.session_state.get("ai_last_call_ts", 0.0))
        wait = max(0.0, float(min_interval_sec) - (now - last))
        if wait > 0:
            _sleep_with_ui(wait, ui_slot)

        st.session_state["ai_last_call_ts"] = time.monotonic()
        res = call_qwen_vl_extract_amount_multi(image_urls, expected_amount, api_key, model)

        reason = str(res.get("reason", "") or "")
        status_code = res.get("status_code", None)

        need_retry = False
        if status_code in (429, 503, 502, 504):
            need_retry = True
        if _looks_like_rate_limited(reason):
            need_retry = True
        if reason == "è¾“å‡ºéJSON":
            need_retry = True

        if (not need_retry) or (attempt >= int(max_retries)):
            return res

        sleep_s = float(backoff_base_sec) * (2 ** attempt)
        sleep_s = min(sleep_s, 15.0)
        attempt += 1
        _sleep_with_ui(sleep_s, ui_slot)


def _call_qwen_vl_extract_amount_multi_with_rl_worker(
    image_urls: List[str],
    expected_amount: float,
    api_key: str,
    model: str,
    last_call_ts: float,
    min_interval_sec: float = 0.8,
    max_retries: int = 4,
    backoff_base_sec: float = 1.0,
) -> Tuple[Dict[str, Any], float]:
    """
    åå°çº¿ç¨‹ç‰ˆï¼šä¸ä¾èµ– Streamlit session_stateã€‚
    è¿”å› (ç»“æœ, æ›´æ–°åçš„ last_call_ts)ã€‚
    """
    attempt = 0
    while True:
        now_m = time.monotonic()
        wait = max(0.0, float(min_interval_sec) - (now_m - float(last_call_ts)))
        if wait > 0:
            time.sleep(wait)

        last_call_ts = time.monotonic()
        res = call_qwen_vl_extract_amount_multi(image_urls, expected_amount, api_key, model)

        reason = str(res.get("reason", "") or "")
        status_code = res.get("status_code", None)

        need_retry = False
        if status_code in (429, 503, 502, 504):
            need_retry = True
        if _looks_like_rate_limited(reason):
            need_retry = True
        if reason == "è¾“å‡ºéJSON":
            need_retry = True

        if (not need_retry) or (attempt >= int(max_retries)):
            return res, last_call_ts

        sleep_s = float(backoff_base_sec) * (2 ** attempt)
        sleep_s = min(sleep_s, 15.0)
        attempt += 1
        time.sleep(sleep_s)


def _process_ai_task_one_row(task: Dict[str, Any], api_key: str, worker_token: str, last_call_ts: float) -> float:
    df_work = task.get("df_work")
    if not isinstance(df_work, pd.DataFrame) or df_work.empty:
        raise RuntimeError("ä»»åŠ¡æ•°æ®ä¸ºç©ºï¼Œæ— æ³•å¤„ç†ã€‚")

    idx = int(task.get("next_idx", 0))
    total = int(task.get("total", len(df_work)))
    if idx >= total:
        return last_call_ts

    col_amount = str(task.get("col_amount", ""))
    col_shot = str(task.get("col_shot", ""))
    model_name = str(task.get("model_name", DEFAULT_VL_MODEL))
    max_images = int(task.get("max_images", 4))
    min_interval_sec = float(task.get("min_interval_sec", 0.8))
    max_retries = int(task.get("max_retries", 4))
    backoff_base_sec = float(task.get("backoff_base_sec", 1.0))

    row = df_work.iloc[idx]
    expected = parse_money(row.get(col_amount))

    paid_amount = None
    is_match = False
    note = ""

    if expected is None:
        note = "é‡‘é¢å­—æ®µæ— æ³•è§£æä¸ºæ•°å­—ï¼ˆè¯·å…ˆå›åˆ°æ­¥éª¤ä¸€æ£€æŸ¥/ä¿®æ­£ï¼‰"
    else:
        raw_cell = row.get(col_shot + HYPERLINK_SUFFIX) or row.get(col_shot)
        img_urls = extract_image_urls_from_cell_value(raw_cell, max_images=max_images)

        # å…œåº•ï¼šå•å…ƒæ ¼åŸå€¼æ˜¯ http ä½†æœªè¢«æ­£åˆ™æå–åˆ°ã€‚
        if not img_urls and isinstance(raw_cell, str) and raw_cell.strip().startswith("http"):
            expanded = normalize_preview_url(raw_cell.strip())
            img_urls = [u for u in expanded if u.lower().endswith(IMAGE_EXTENSIONS)]
            img_urls = img_urls[:max_images] if img_urls else expanded[:max_images]

        if not img_urls:
            res = {
                "paid_amount": None,
                "is_match": None,
                "reason": "æœªæ‰¾åˆ°å¯ç”¨å›¾ç‰‡URLï¼ˆé¢„è§ˆé“¾æ¥æœªè§£æå‡ºå›¾ç‰‡ï¼‰",
                "raw_text": "",
            }
        else:
            res, last_call_ts = _call_qwen_vl_extract_amount_multi_with_rl_worker(
                img_urls,
                float(expected),
                api_key=api_key,
                model=model_name,
                last_call_ts=last_call_ts,
                min_interval_sec=min_interval_sec,
                max_retries=max_retries,
                backoff_base_sec=backoff_base_sec,
            )

        paid_amount = res.get("paid_amount")
        is_match = (res.get("is_match") is True)
        note = "" if is_match else (res.get("reason") or "AIåˆ¤å®šå¼‚å¸¸")

    df_work.at[idx, COL_AI_EXTRACTED_AMOUNT] = paid_amount
    df_work.at[idx, COL_AI_MATCH] = bool(is_match)
    df_work.at[idx, COL_AI_NOTE] = note
    df_work.at[idx, COL_AI_TASK_STATUS] = AI_ROW_STATUS_DONE
    df_work.at[idx, COL_AI_TASK_UPDATED_AT] = now_iso()

    task["df_work"] = df_work
    task["next_idx"] = idx + 1
    task["error_message"] = ""
    task["worker_token"] = worker_token
    if int(task["next_idx"]) >= int(task.get("total", len(df_work))):
        task["status"] = AI_TASK_STATUS_COMPLETED
        task["finished_at"] = now_iso()
    save_ai_task_state(task)
    return last_call_ts


def ai_task_worker_loop(task_id: str, api_key: str, worker_token: str) -> None:
    last_call_ts = 0.0
    while True:
        task = load_ai_task_state(task_id)
        if not task:
            return

        if str(task.get("worker_token", "")) != str(worker_token):
            return

        status = str(task.get("status", ""))
        if status != AI_TASK_STATUS_RUNNING:
            return

        try:
            total = int(task.get("total", 0))
            next_idx = int(task.get("next_idx", 0))
            if next_idx >= total:
                task["status"] = AI_TASK_STATUS_COMPLETED
                task["finished_at"] = now_iso()
                save_ai_task_state(task)
                return

            last_call_ts = _process_ai_task_one_row(task, api_key=api_key, worker_token=worker_token, last_call_ts=last_call_ts)
            time.sleep(0.01)

        except Exception as e:
            task["status"] = AI_TASK_STATUS_ERROR
            task["error_message"] = str(e)
            task["finished_at"] = now_iso()
            save_ai_task_state(task)
            return


def start_ai_task_worker(task_id: str, api_key: str) -> bool:
    if not task_id or not api_key:
        return False

    task = load_ai_task_state(task_id)
    if not task:
        return False

    worker_token = f"w_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
    task["worker_token"] = worker_token
    task["status"] = AI_TASK_STATUS_RUNNING
    task["error_message"] = ""
    save_ai_task_state(task)

    t = threading.Thread(
        target=ai_task_worker_loop,
        args=(task_id, api_key, worker_token),
        daemon=True,
        name=f"ai_task_{task_id[-8:]}",
    )
    t.start()
    return True


def finalize_ai_task_if_needed(task: Dict[str, Any]) -> Dict[str, Any]:
    if not isinstance(task, dict):
        return task
    if str(task.get("status", "")) != AI_TASK_STATUS_COMPLETED:
        return task
    if bool(task.get("history_logged", False)):
        return task

    frames = split_ai_task_frames(task)
    df_ok = frames["ok"]
    df_bad = frames["bad"]
    df_pending = frames["pending"]

    col_shot = str(task.get("col_shot", ""))
    hyperlink_ok = [col_shot] if col_shot and col_shot in df_ok.columns else None
    hyperlink_bad = [col_shot] if col_shot and col_shot in df_bad.columns else None
    hyperlink_pending = [col_shot] if col_shot and col_shot in df_pending.columns else None

    ts = now_ts()
    ok_name = f"{ts}_AIå¤æ ¸æ­£å¸¸_å¾…æ‰“æ¬¾.xlsx"
    bad_name = f"{ts}_AIå¤æ ¸å¼‚å¸¸_éœ€äººå·¥.xlsx"
    pending_name = f"{ts}_AIæœªå¤„ç†_å¾…ç»§ç»­.xlsx"

    b_ok = df_to_excel_bytes(df_ok, sheet_name="AIå¤æ ¸æ­£å¸¸", hyperlink_cols=hyperlink_ok)
    b_bad = df_to_excel_bytes(df_bad, sheet_name="AIå¤æ ¸å¼‚å¸¸", hyperlink_cols=hyperlink_bad)
    b_pending = df_to_excel_bytes(df_pending, sheet_name="AIæœªå¤„ç†", hyperlink_cols=hyperlink_pending) if not df_pending.empty else b""

    ok_artifact = save_artifact_bytes("step3_ai_ok", ok_name, b_ok)
    bad_artifact = save_artifact_bytes("step3_ai_bad", bad_name, b_bad)
    pending_artifact = save_artifact_bytes("step3_ai_pending", pending_name, b_pending) if b_pending else ""

    source_df = task.get("source_df")
    if not isinstance(source_df, pd.DataFrame) or source_df.empty:
        source_df = task.get("df_work", pd.DataFrame()).copy()
    report_step3 = compare_source_and_processed(source_df, task.get("df_work", pd.DataFrame()), stage_name="æ­¥éª¤ä¸‰AIå¤æ ¸")

    append_operation_history(
        stage="æ­¥éª¤ä¸‰AIå¤æ ¸",
        action="AIä»»åŠ¡å®Œæˆ",
        detail={
            "task_id": task.get("task_id", ""),
            "source_file": task.get("source_file", ""),
            "input_rows": int(task.get("input_rows", 0)),
            "processed_rows": int(task.get("next_idx", 0)),
            "output_rows": len(task.get("df_work", pd.DataFrame())),
            "ai_ok_rows": len(df_ok),
            "ai_bad_rows": len(df_bad),
            "ai_pending_rows": len(df_pending),
            "alignment_can_compare": report_step3.get("can_compare"),
            "alignment_ok": report_step3.get("ok") if report_step3.get("can_compare") else None,
            "alignment_missing_rows": report_step3.get("missing_rows"),
            "alignment_extra_rows": report_step3.get("extra_rows"),
            "artifacts": [p for p in [ok_artifact, bad_artifact, pending_artifact] if p],
        }
    )

    task["history_logged"] = True
    task["alignment_report"] = report_step3
    task["artifacts"] = [p for p in [ok_artifact, bad_artifact, pending_artifact] if p]
    save_ai_task_state(task)
    return load_ai_task_state(str(task.get("task_id", ""))) or task


# =============================================================================
# ã€5ã€‘Streamlit é¡µé¢
# =============================================================================

# é˜²æ­¢éƒ¨ç½²æ—¶è„šæœ¬è¢«é‡å¤æ‹¼æ¥/æ‰§è¡Œï¼Œå¯¼è‡´æ ‡é¢˜ä¸æ§ä»¶é‡å¤æ¸²æŸ“
if globals().get("_REFUND_APP_RENDERED_ONCE", False):
    st.stop()
globals()["_REFUND_APP_RENDERED_ONCE"] = True

st.set_page_config(page_title="é€€è¿è´¹æ™ºèƒ½å®¡æ ¸ç³»ç»Ÿï½œå çº¸å¿ƒæ„æ——èˆ°åº—", layout="wide")
st.title("ğŸ§¾ é€€è¿è´¹æ™ºèƒ½å®¡æ ¸ç³»ç»Ÿï¼ˆå†…éƒ¨ææ•ˆï¼‰")
st.caption("Streamlit + Pandas + é€šä¹‰åƒé—®-VLï¼ˆDashScopeï¼‰")

st.markdown(
    """
<style>
@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;500;700;900&display=swap');

:root {
  --ui-primary: #0f5ea8;
  --ui-secondary: #0f766e;
  --ui-accent: #ea580c;
  --ui-bg: #f5f7fb;
  --ui-card: #ffffff;
  --ui-border: #d8e1ee;
  --ui-text: #0f172a;
  --ui-muted: #475569;
}

html, body, [class*="css"] {
  font-family: "Noto Sans SC", "PingFang SC", "Microsoft YaHei", sans-serif;
}

.stApp {
  background:
    radial-gradient(1100px 360px at -8% -12%, rgba(15, 94, 168, 0.17), transparent 56%),
    radial-gradient(900px 280px at 108% -8%, rgba(15, 118, 110, 0.13), transparent 58%),
    var(--ui-bg);
}

.block-container {
  max-width: 1360px;
  padding-top: 1.2rem;
}

.hero-panel {
  background: linear-gradient(120deg, rgba(15, 94, 168, 0.95), rgba(15, 118, 110, 0.92));
  color: #fff;
  border-radius: 18px;
  padding: 20px 24px;
  margin: 0.2rem 0 1rem 0;
  box-shadow: 0 14px 40px rgba(15, 94, 168, 0.24);
}

.hero-title {
  font-size: 1.32rem;
  font-weight: 900;
  letter-spacing: 0.2px;
  margin-bottom: 0.2rem;
}

.hero-sub {
  font-size: 0.94rem;
  opacity: 0.95;
}

div[data-testid="stTabs"] button {
  border-radius: 999px;
  border: 1px solid var(--ui-border);
  padding: 0.4rem 0.95rem;
  margin-right: 0.32rem;
  color: var(--ui-text);
}

div[data-testid="stTabs"] button[aria-selected="true"] {
  background: linear-gradient(120deg, var(--ui-primary), #1870c0);
  color: #fff;
  border-color: transparent;
  box-shadow: 0 8px 22px rgba(15, 94, 168, 0.28);
}

div[data-testid="stMetric"] {
  background: var(--ui-card);
  border: 1px solid var(--ui-border);
  border-radius: 14px;
  padding: 12px 14px;
  box-shadow: 0 6px 20px rgba(15, 23, 42, 0.06);
}

.ops-flow {
  display: grid;
  grid-template-columns: repeat(3, minmax(0, 1fr));
  gap: 10px;
  margin-top: 0.3rem;
}

.ops-item {
  background: var(--ui-card);
  border: 1px solid var(--ui-border);
  border-radius: 14px;
  padding: 12px 14px;
}

.ops-kicker {
  color: var(--ui-primary);
  font-weight: 800;
  font-size: 0.78rem;
}

.ops-title {
  color: var(--ui-text);
  font-weight: 800;
  margin-top: 4px;
  margin-bottom: 4px;
}

.ops-desc {
  color: var(--ui-muted);
  font-size: 0.86rem;
  line-height: 1.45;
}

.stButton > button, .stDownloadButton > button {
  border-radius: 12px;
  border: 1px solid #c7d4e8;
  font-weight: 700;
}
</style>
    """,
    unsafe_allow_html=True
)

st.markdown(
    """
<div class="hero-panel">
  <div class="hero-title">é€€è¿è´¹æ™ºèƒ½å®¡æ ¸ä¸­å°</div>
  <div class="hero-sub">å›´ç»•ã€Œæ¸…æ´— â†’ å…¥åº“åŒ¹é… â†’ AIå¤æ ¸ã€æ„å»ºé—­ç¯å®¡æ ¸æµï¼Œæ”¯æŒåå°ä»»åŠ¡ã€å¯è¿½æº¯å†å²ä¸åˆ†æ®µä¸‹è½½ã€‚</div>
</div>
    """,
    unsafe_allow_html=True
)

main_tabs = st.tabs([
    "ğŸ  æ€»è§ˆçœ‹æ¿",
    "1ï¸âƒ£ æ­¥éª¤ä¸€ï¼šæ•°æ®æ¸…æ´—",
    "2ï¸âƒ£ æ­¥éª¤äºŒ&ä¸‰ï¼šå…¥åº“ + AIå¤æ ¸",
    "ğŸ—‚ï¸ å†å²ä¸­å¿ƒ",
])

with main_tabs[0]:
    step1_normal_count = len(st.session_state.get("tab1_normal_df", pd.DataFrame())) if isinstance(st.session_state.get("tab1_normal_df"), pd.DataFrame) else 0
    step1_abnormal_count = len(st.session_state.get("tab1_abnormal_df", pd.DataFrame())) if isinstance(st.session_state.get("tab1_abnormal_df"), pd.DataFrame) else 0
    step2_inbound_count = len(st.session_state.get("step2_inbound_df", pd.DataFrame())) if isinstance(st.session_state.get("step2_inbound_df"), pd.DataFrame) else 0
    step2_pending_count = len(st.session_state.get("step2_not_inbound_df", pd.DataFrame())) if isinstance(st.session_state.get("step2_not_inbound_df"), pd.DataFrame) else 0

    active_task = None
    active_task_id_from_state = str(st.session_state.get("active_ai_task_id", "")).strip()
    if active_task_id_from_state:
        active_task = load_ai_task_state(active_task_id_from_state)
    if active_task is None:
        active_task = load_latest_ai_task_state(prefer_active=True)

    ai_processed_rows = 0
    ai_pending_rows = 0
    ai_task_status = "æš‚æ— ä»»åŠ¡"
    if isinstance(active_task, dict):
        ai_summary = summarize_ai_task(active_task)
        ai_processed_rows = int(ai_summary.get("processed_rows", 0))
        ai_pending_rows = int(ai_summary.get("pending_rows", 0))
        ai_task_status = task_status_label(str(active_task.get("status", "")))

    history_count = len(load_operation_history_df())
    artifact_count = len(load_artifact_catalog_df())

    o1, o2, o3, o4, o5, o6 = st.columns(6)
    o1.metric("æ¸…æ´—æ­£å¸¸", step1_normal_count)
    o2.metric("æ¸…æ´—å¼‚å¸¸", step1_abnormal_count)
    o3.metric("å·²å…¥åº“å¾…AI", step2_inbound_count)
    o4.metric("æœªå…¥åº“å¾…è·Ÿè¿›", step2_pending_count)
    o5.metric("AIå·²å¤„ç†", ai_processed_rows)
    o6.metric("AIæœªå¤„ç†", ai_pending_rows)
    st.caption(f"å½“å‰AIä»»åŠ¡çŠ¶æ€ï¼š{ai_task_status}ï½œå†å²è®°å½•ï¼š{history_count} æ¡ï½œå†å²è¡¨æ ¼ï¼š{artifact_count} ä»½")

    st.markdown(
        """
<div class="ops-flow">
  <div class="ops-item">
    <div class="ops-kicker">STEP 01</div>
    <div class="ops-title">åŸºç¡€æ¸…æ´—ä¸è§„åˆ™åˆç­›</div>
    <div class="ops-desc">å…ˆæ¸…æ´—ç­ç‰›ç™»è®°è¡¨ï¼ŒæŒ‰é‡‘é¢ã€è´¦å·ã€å®åã€ç‰©æµå•å·åšè§„åˆ™æ ¡éªŒï¼Œä¼˜å…ˆå¤„ç†å¼‚å¸¸å›è®¿ã€‚</div>
  </div>
  <div class="ops-item">
    <div class="ops-kicker">STEP 02</div>
    <div class="ops-title">å…¥åº“åŒ¹é…ä¸æ¨è¿›åˆ†æµ</div>
    <div class="ops-desc">å°†æ­£å¸¸æ•°æ®ä¸å·²å…¥åº“å•å·åŒ¹é…ï¼Œç”Ÿæˆã€Œå¯è¿›AIã€ä¸ã€Œå¾…è·Ÿè¿›ã€ä¸¤æ¡å¹¶è¡Œå¤„ç†è·¯å¾„ã€‚</div>
  </div>
  <div class="ops-item">
    <div class="ops-kicker">STEP 03</div>
    <div class="ops-title">AIåå°å¤æ ¸ä¸ç»“æœè½æ¡£</div>
    <div class="ops-desc">AIä»»åŠ¡æ”¯æŒåå°è¿è¡Œã€æš‚åœ/ç»§ç»­ï¼Œå®æ—¶çœ‹è¿›åº¦ï¼Œéšæ—¶ä¸‹è½½å·²å¤„ç†ä¸æœªå¤„ç†åˆ†æ®µç»“æœã€‚</div>
  </div>
</div>
        """,
        unsafe_allow_html=True
    )

    with st.expander("ğŸ“Œ ä½¿ç”¨è¯´æ˜ï¼ˆå±•å¼€æŸ¥çœ‹ï¼‰", expanded=False):
        st.markdown(
            """
- **æ­¥éª¤ä¸€ï¼ˆæ¸…æ´—ï¼‰**ï¼šä¸Šä¼ ç­ç‰›ç™»è®°è¡¨ â†’ æ¸…æ´— & è§„åˆ™åˆç­› â†’ ä¸‹è½½ã€æ­£å¸¸/å¼‚å¸¸ã€‘
  - âœ… å…ˆå¤„ç†å¼‚å¸¸å›è®¿ï¼Œå†æ¨è¿›åç»­æµç¨‹
- **æ­¥éª¤äºŒï¼ˆå…¥åº“åŒ¹é…ï¼‰**ï¼šä¸Šä¼ ã€æ­¥éª¤ä¸€æ­£å¸¸è¡¨/å›è®¿åæ­£å¸¸è¡¨ã€‘+ã€å·²å…¥åº“ç‰©æµå•å·è¡¨ã€‘â†’ åŒ¹é…å…¥åº“çŠ¶æ€ â†’ ä¸‹è½½ã€å·²å…¥åº“å¾…AI/æœªå…¥åº“å¾…è·Ÿè¿›ã€‘
- **æ­¥éª¤ä¸‰ï¼ˆAIå¤æ ¸ï¼Œå¤šå›¾ï¼‰**ï¼šä¼˜å…ˆä½¿ç”¨æ­¥éª¤äºŒçš„ã€å·²å…¥åº“å¾…AIã€‘â†’ ä»â€œé¢„è§ˆé“¾æ¥â€æ‹†å‡ºå¤šå¼ å›¾ç‰‡ â†’ AIæ ¸å¯¹è¿è´¹é‡‘é¢ â†’ ä¸‹è½½ã€AIæ­£å¸¸/AIå¼‚å¸¸ã€‘
  - âœ… æ”¯æŒåå°ä»»åŠ¡ï¼šå¯æš‚åœ/ç»§ç»­ï¼Œåˆ·æ–°åè‡ªåŠ¨æ¢å¤ä»»åŠ¡çŠ¶æ€ï¼Œå¹¶å¯éšæ—¶ä¸‹è½½ã€å·²å¤„ç†æ­£å¸¸/å·²å¤„ç†å¼‚å¸¸/æœªå¤„ç†ã€‘åˆ†æ®µç»“æœ

âœ… è¶…é“¾æ¥ä¿®å¤ï¼šè¯»å–æ—¶æŠ½å– URLï¼ˆå…¼å®¹ hyperlink / å…¬å¼ / tooltip / æ‰¹æ³¨ï¼‰ï¼Œå¯¼å‡ºæ—¶å†™å› hyperlinkï¼Œä¸æ”¹å•å…ƒæ ¼æ–‡å­—ã€‚  
âœ… AI é™é€Ÿï¼šæ”¯æŒæœ€å°è¯·æ±‚é—´éš” + é™æµ/ç¹å¿™è‡ªåŠ¨é€€é¿é‡è¯•ã€‚  
âœ… å¤šå›¾ AIï¼šæ”¯æŒé¢„è§ˆé“¾æ¥é‡Œå¤šå¼ å›¾ç‰‡ï¼Œæ¨¡å‹åœ¨å¤šå›¾ä¸­è‡ªåŠ¨å¯»æ‰¾è¿è´¹å­—æ®µå¯¹åº”é‡‘é¢ï¼Œé™ä½â€œæœªæ˜¾ç¤ºè¿è´¹é‡‘é¢â€è¯¯åˆ¤ã€‚
            """
        )

with main_tabs[3]:
    st.subheader("æ“ä½œå†å²ä¸å­˜æ¡£ä¸­å¿ƒ")
    st.caption("æŒ‰å¹´æœˆæ—¥ç­›é€‰æ“ä½œæ—¥å¿—ï¼Œæ”¯æŒæŸ¥çœ‹å¹¶ä¸‹è½½å†å²è¿‡ç¨‹äº§ç‰©ã€‚")
    if st.session_state.get("history_write_error"):
        st.warning(f"âš ï¸ å†å²è®°å½•å†™å…¥å¼‚å¸¸ï¼š{st.session_state.get('history_write_error')}")
    if st.session_state.get("history_read_error"):
        st.warning(f"âš ï¸ å†å²è®°å½•è¯»å–å¼‚å¸¸ï¼š{st.session_state.get('history_read_error')}")

    hist_df = load_operation_history_df()
    if hist_df.empty:
        st.info("æš‚æ— å†å²è®°å½•ã€‚æ‰§è¡Œæ­¥éª¤ä¸€/æ­¥éª¤äºŒ/æ­¥éª¤ä¸‰åä¼šè‡ªåŠ¨è®°å½•ã€‚")
    else:
        for c in ["year", "month", "day"]:
            if c in hist_df.columns:
                hist_df[c] = hist_df[c].astype(str)
                hist_df[c] = hist_df[c].replace({"nan": "", "None": ""})

        year_options = ["å…¨éƒ¨"] + sorted([x for x in hist_df["year"].dropna().unique().tolist() if str(x).strip() != ""], reverse=True) if "year" in hist_df.columns else ["å…¨éƒ¨"]
        sel_year = st.selectbox("å¹´ä»½", options=year_options, index=0, key=unique_widget_key("history_main_filter_year"))

        filtered_df = hist_df.copy()
        if sel_year != "å…¨éƒ¨" and "year" in filtered_df.columns:
            filtered_df = filtered_df[filtered_df["year"] == sel_year]

        month_options = ["å…¨éƒ¨"] + sorted([x for x in filtered_df["month"].dropna().unique().tolist() if str(x).strip() != ""], reverse=True) if "month" in filtered_df.columns else ["å…¨éƒ¨"]
        sel_month = st.selectbox("æœˆä»½", options=month_options, index=0, key=unique_widget_key("history_main_filter_month"))
        if sel_month != "å…¨éƒ¨" and "month" in filtered_df.columns:
            filtered_df = filtered_df[filtered_df["month"] == sel_month]

        day_options = ["å…¨éƒ¨"] + sorted([x for x in filtered_df["day"].dropna().unique().tolist() if str(x).strip() != ""], reverse=True) if "day" in filtered_df.columns else ["å…¨éƒ¨"]
        sel_day = st.selectbox("æ—¥æœŸ", options=day_options, index=0, key=unique_widget_key("history_main_filter_day"))
        if sel_day != "å…¨éƒ¨" and "day" in filtered_df.columns:
            filtered_df = filtered_df[filtered_df["day"] == sel_day]

        st.caption(f"å½“å‰ç­›é€‰ç»“æœï¼š{len(filtered_df)} æ¡è®°å½•ã€‚")
        show_cols = [c for c in [
            "timestamp", "stage", "action", "task_id", "operator", "source_file", "input_rows",
            "output_rows", "normal_rows", "abnormal_rows", "inbound_rows", "pending_rows",
            "ai_ok_rows", "ai_bad_rows", "ai_pending_rows", "alignment_ok", "alignment_missing_rows", "alignment_extra_rows", "artifacts"
        ] if c in filtered_df.columns]
        if not show_cols:
            show_cols = filtered_df.columns.tolist()

        st.dataframe(filtered_df[show_cols], use_container_width=True, height=340)

        csv_bytes = filtered_df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
        excel_bytes = df_to_excel_bytes(filtered_df, sheet_name="æ“ä½œå†å²", hyperlink_cols=None)

        d1, d2 = st.columns(2)
        with d1:
            st.download_button(
                "â¬‡ï¸ ä¸‹è½½ç­›é€‰å†å²ï¼ˆCSVï¼‰",
                data=csv_bytes,
                file_name=build_history_download_name("æ“ä½œå†å²_ç­›é€‰ç»“æœ"),
                mime="text/csv"
            )
        with d2:
            st.download_button(
                "â¬‡ï¸ ä¸‹è½½ç­›é€‰å†å²ï¼ˆExcelï¼‰",
                data=excel_bytes,
                file_name=build_history_download_name("æ“ä½œå†å²_ç­›é€‰ç»“æœ").replace(".csv", ".xlsx"),
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

        st.divider()
        st.markdown("#### ğŸ“¦ å†å²è¡¨æ ¼æ–‡ä»¶ï¼ˆå¯ä¸‹è½½ï¼‰")
        artifact_df = load_artifact_catalog_df()
        if artifact_df.empty:
            st.info("æš‚æ— å·²å­˜æ¡£è¡¨æ ¼ã€‚å®Œæˆæ­¥éª¤ä¸€/æ­¥éª¤äºŒ/æ­¥éª¤ä¸‰åä¼šè‡ªåŠ¨å­˜æ¡£ã€‚")
        else:
            for c in ["year", "month", "day"]:
                if c in artifact_df.columns:
                    artifact_df[c] = artifact_df[c].astype(str)

            filtered_artifacts = artifact_df.copy()
            if sel_year != "å…¨éƒ¨":
                filtered_artifacts = filtered_artifacts[filtered_artifacts["year"] == sel_year]
            if sel_month != "å…¨éƒ¨":
                filtered_artifacts = filtered_artifacts[filtered_artifacts["month"] == sel_month]
            if sel_day != "å…¨éƒ¨":
                filtered_artifacts = filtered_artifacts[filtered_artifacts["day"] == sel_day]

            st.caption(f"å½“å‰ç­›é€‰å‘½ä¸­çš„å†å²è¡¨æ ¼ï¼š{len(filtered_artifacts)} ä»½ã€‚")
            st.dataframe(
                filtered_artifacts[["timestamp", "stage_key", "file_name", "size_kb", "file_path"]],
                use_container_width=True,
                height=260
            )

            if not filtered_artifacts.empty:
                file_options = filtered_artifacts["file_path"].tolist()
                selected_path = st.selectbox("é€‰æ‹©è¦ä¸‹è½½çš„å†å²è¡¨æ ¼", options=file_options, key=unique_widget_key("history_artifact_select_path"))
                selected_abs = Path.cwd() / selected_path
                selected_display_name = filtered_artifacts.loc[
                    filtered_artifacts["file_path"] == selected_path, "file_name"
                ].iloc[0]

                if selected_abs.exists():
                    st.download_button(
                        "â¬‡ï¸ ä¸‹è½½æ‰€é€‰å†å²è¡¨æ ¼",
                        data=selected_abs.read_bytes(),
                        file_name=selected_display_name,
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

                zip_buf = BytesIO()
                with zipfile.ZipFile(zip_buf, "w", compression=zipfile.ZIP_DEFLATED) as zf:
                    for _, row in filtered_artifacts.iterrows():
                        rel = str(row.get("file_path", ""))
                        p = Path.cwd() / rel
                        if p.exists():
                            arcname = f"{row.get('timestamp', '').replace(':', '').replace(' ', '_')}__{row.get('file_name', p.name)}"
                            zf.write(p, arcname=arcname)
                st.download_button(
                    "â¬‡ï¸ æ‰“åŒ…ä¸‹è½½ç­›é€‰å†å²è¡¨æ ¼ï¼ˆZIPï¼‰",
                    data=zip_buf.getvalue(),
                    file_name=f"{now_ts()}_å†å²è¡¨æ ¼.zip",
                    mime="application/zip"
                )

# Sidebarï¼šAI é…ç½®
st.sidebar.header("ğŸ” AI é…ç½®")
default_key = get_dashscope_api_key()
api_key_input = st.sidebar.text_input("DashScope API Keyï¼ˆsecrets/envä¼˜å…ˆï¼‰", value=default_key or "", type="password")
model_name = st.sidebar.text_input("VL æ¨¡å‹", value=DEFAULT_VL_MODEL)

st.sidebar.subheader("ğŸ–¼ï¸ å¤šå›¾è¯†åˆ«é…ç½®")
max_images_per_row = st.sidebar.number_input("æ¯æ¡è®°å½•æœ€å¤šä¼ ç»™ AI çš„å›¾ç‰‡æ•°", min_value=1, max_value=10, value=4, step=1)

st.sidebar.subheader("â±ï¸ AI é€Ÿç‡é™åˆ¶ï¼ˆå»ºè®®å¼€å¯ï¼‰")
min_interval_sec = st.sidebar.number_input("æ¯æ¬¡è¯·æ±‚æœ€å°é—´éš”ï¼ˆç§’ï¼‰", min_value=0.0, max_value=10.0, value=0.8, step=0.1)
max_retries = st.sidebar.number_input("é™æµ/ç¹å¿™æ—¶æœ€å¤§é‡è¯•æ¬¡æ•°", min_value=0, max_value=10, value=4, step=1)
backoff_base_sec = st.sidebar.number_input("é€€é¿åŸºå‡†ç§’æ•°ï¼ˆæŒ‡æ•°é€€é¿ï¼‰", min_value=0.1, max_value=5.0, value=1.0, step=0.1)

max_ai_rows = st.sidebar.number_input("AI æœ€å¤§å¤„ç†è¡Œæ•°ï¼ˆé˜²è¯¯ç‚¹ï¼‰", min_value=1, max_value=5000, value=300, step=50)


# =============================================================================
# Tab2ï¼šæ­¥éª¤äºŒå…¥åº“åŒ¹é… + æ­¥éª¤ä¸‰AIå¤æ ¸ï¼ˆå¤šå›¾ï¼‰
# =============================================================================
with main_tabs[2]:
    st.subheader("æ­¥éª¤äºŒï¼ˆAï¼‰ï¼šå…¥åº“åŒ¹é…ï¼ˆå…ˆæ¸…æ´—ï¼Œå†åŒ¹é…ï¼‰")
    st.info("è¯´æ˜ï¼šè¯·å…ˆå®Œæˆæ­¥éª¤ä¸€æ¸…æ´—ä¸å¼‚å¸¸å›è®¿ï¼Œå†åœ¨è¿™é‡ŒæŠŠã€æ­£å¸¸è¡¨ã€‘å’Œã€å·²å…¥åº“ç‰©æµå•å·è¡¨ã€‘è¿›è¡ŒåŒ¹é…ã€‚")

    source_left, source_right = st.columns([2, 1])
    with source_left:
        uploaded_step2_source = st.file_uploader(
            "ä¸Šä¼ ã€æ­¥éª¤ä¸€æ­£å¸¸è¡¨/å›è®¿åæ­£å¸¸è¡¨ã€‘ï¼ˆ.xlsx / .xls / .csvï¼‰",
            type=["xlsx", "xls", "csv"],
            key="step2_source_uploader"
        )
    with source_right:
        use_tab1_normal_for_step2 = st.checkbox(
            "ç›´æ¥ä½¿ç”¨æ­¥éª¤ä¸€çš„ã€æ­£å¸¸è¡¨ã€‘ï¼ˆè‹¥å·²ç”Ÿæˆï¼‰",
            value=True,
            key="use_tab1_normal_for_step2"
        )

    df_step2_source = None
    step2_source_bytes = b""

    if use_tab1_normal_for_step2 and isinstance(st.session_state.get("tab1_normal_df"), pd.DataFrame):
        df_step2_source = st.session_state["tab1_normal_df"].copy()
        st.success("å·²è½½å…¥æ­¥éª¤ä¸€æ­£å¸¸è¡¨ï¼ˆå½“å‰ä¼šè¯ï¼‰ã€‚")
        render_preview_dataframe(
            df_step2_source,
            title="æ­¥éª¤äºŒå¾…åŒ¹é…æ•°æ®é¢„è§ˆï¼ˆæ¥è‡ªæ­¥éª¤ä¸€ï¼‰",
            key_prefix="tab2_step1_session_source",
            default_rows=50,
            height=340,
            expanded=False,
        )
    elif uploaded_step2_source is not None:
        try:
            step2_source_bytes = get_uploaded_bytes(uploaded_step2_source)
            df_step2_source = read_table(uploaded_step2_source)

            shot_col_step2_source = find_first_existing_column(df_step2_source, COL_SCREENSHOT_CANDIDATES)
            if shot_col_step2_source and uploaded_step2_source.name.lower().endswith((".xlsx", ".xls")):
                df_step2_source = attach_hyperlink_helper_column(df_step2_source, step2_source_bytes, shot_col_step2_source)

            render_preview_dataframe(
                df_step2_source,
                title="æ­¥éª¤äºŒå¾…åŒ¹é…æ•°æ®é¢„è§ˆ",
                key_prefix="tab2_uploaded_source",
                default_rows=50,
                height=340,
                expanded=False,
            )
        except Exception as e:
            st.error(f"âŒ æ­¥éª¤äºŒå¾…åŒ¹é…æ•°æ®è¯»å–å¤±è´¥ï¼š{e}")
            df_step2_source = None

    inbound_uploader = st.file_uploader(
        "ä¸Šä¼ ã€å·²å…¥åº“ç‰©æµå•å·è¡¨ã€‘ï¼ˆ.xlsx / .xls / .csvï¼‰",
        type=["xlsx", "xls", "csv"],
        key="inbound_uploader"
    )

    inbound_set = st.session_state.get("inbound_logistics_set", set())
    inbound_col_name = st.session_state.get("inbound_logistics_col_name", "")

    if inbound_uploader is not None:
        try:
            df_inbound = read_table(inbound_uploader)
            if df_inbound.empty:
                st.warning("è¯»å–åˆ°çš„å…¥åº“è¡¨ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹ã€‚")
            else:
                render_preview_dataframe(
                    df_inbound,
                    title="å·²è¯»å–å…¥åº“è¡¨é¢„è§ˆ",
                    key_prefix="tab2_uploaded_inbound",
                    default_rows=50,
                    height=320,
                    expanded=False,
                )

                required_inb = {"å·²å…¥åº“ç‰©æµå•å·": COL_LOGISTICS_NO_CANDIDATES}
                matched_inb = ensure_required_columns(df_inbound, required_inb)
                inb_col = matched_inb["å·²å…¥åº“ç‰©æµå•å·"]

                inbound_set = build_inbound_set(df_inbound, inb_col)
                st.session_state["inbound_logistics_set"] = inbound_set
                st.session_state["inbound_logistics_col_name"] = inb_col
                inbound_col_name = inb_col

                st.success(f"âœ… å·²ç¼“å­˜å…¥åº“å•å· {len(inbound_set)} ä¸ªï¼ˆåˆ—ï¼š{inb_col}ï¼‰ã€‚")

        except ValueError as ve:
            st.error(f"âŒ å…¥åº“è¡¨ç¼ºå°‘å¿…è¦åˆ—ï¼š\n\n{ve}")
        except Exception as e:
            st.error(f"âŒ å…¥åº“è¡¨å¤„ç†å¤±è´¥ï¼š{e}")
            with st.expander("æŸ¥çœ‹é”™è¯¯è¯¦æƒ…ï¼ˆå¼€å‘ç”¨ï¼‰"):
                st.code(traceback.format_exc())

    if inbound_set:
        st.caption(f"å½“å‰ä¼šè¯å·²ç¼“å­˜å…¥åº“å•å·ï¼š{len(inbound_set)} ä¸ªï¼ˆæ¥è‡ªåˆ—ï¼š{inbound_col_name or 'æœªçŸ¥'}ï¼‰")
    else:
        st.caption("å½“å‰ä¼šè¯å°šæœªç¼“å­˜å…¥åº“å•å·ã€‚")

    if use_tab1_normal_for_step2 and isinstance(st.session_state.get("tab1_normal_df"), pd.DataFrame):
        step2_source_label = "æ­¥éª¤ä¸€æ­£å¸¸è¡¨ï¼ˆä¼šè¯ï¼‰"
    elif uploaded_step2_source is not None:
        step2_source_label = uploaded_step2_source.name
    else:
        step2_source_label = ""
    inbound_source_label = inbound_uploader.name if inbound_uploader is not None else ("å·²ç¼“å­˜å…¥åº“å•å·ï¼ˆä¼šè¯ï¼‰" if inbound_set else "")

    run_match = st.button("ğŸ” æ‰§è¡Œå…¥åº“åŒ¹é…", type="primary", key="run_inbound_match")

    if run_match:
        if df_step2_source is None or df_step2_source.empty:
            st.warning("è¯·å…ˆæä¾›æ­¥éª¤äºŒå¾…åŒ¹é…æ•°æ®ï¼ˆæ­¥éª¤ä¸€æ­£å¸¸è¡¨æˆ–ä¸Šä¼ å›è®¿åæ­£å¸¸è¡¨ï¼‰ã€‚")
        elif not inbound_set:
            st.warning("è¯·å…ˆä¸Šä¼ ã€å·²å…¥åº“ç‰©æµå•å·è¡¨ã€‘ã€‚")
        else:
            try:
                required_step2 = {"é€€å›ç‰©æµå•å·": COL_LOGISTICS_NO_CANDIDATES}
                matched_step2 = ensure_required_columns(df_step2_source, required_step2)
                col_lno_step2 = matched_step2["é€€å›ç‰©æµå•å·"]

                df_step2_matched = attach_inbound_flag(df_step2_source, col_lno_step2, inbound_set)
                df_step2_inbound = df_step2_matched[df_step2_matched[COL_INBOUND_FLAG] == "å·²å…¥åº“"].copy()
                df_step2_pending = df_step2_matched[df_step2_matched[COL_INBOUND_FLAG] != "å·²å…¥åº“"].copy()
                report_step2 = compare_source_and_processed(df_step2_source, df_step2_matched, stage_name="æ­¥éª¤äºŒå…¥åº“åŒ¹é…")

                shot_col_step2 = find_first_existing_column(df_step2_matched, COL_SCREENSHOT_CANDIDATES)
                hyperlink_cols_inbound = [shot_col_step2] if shot_col_step2 and shot_col_step2 in df_step2_inbound.columns else None
                hyperlink_cols_pending = [shot_col_step2] if shot_col_step2 and shot_col_step2 in df_step2_pending.columns else None

                ts_step2 = now_ts()
                inbound_name = f"{ts_step2}_å…¥åº“åŒ¹é…é€šè¿‡_å¾…AIå¤æ ¸.xlsx"
                pending_name = f"{ts_step2}_æœªå…¥åº“å¾…è·Ÿè¿›.xlsx"
                b_inbound = df_to_excel_bytes(df_step2_inbound, sheet_name="å·²å…¥åº“", hyperlink_cols=hyperlink_cols_inbound)
                b_pending = df_to_excel_bytes(df_step2_pending, sheet_name="æœªå…¥åº“", hyperlink_cols=hyperlink_cols_pending)

                inbound_artifact = save_artifact_bytes("step2_inbound", inbound_name, b_inbound)
                pending_artifact = save_artifact_bytes("step2_pending", pending_name, b_pending)

                st.session_state["step2_matched_df"] = df_step2_matched
                st.session_state["step2_inbound_df"] = df_step2_inbound
                st.session_state["step2_not_inbound_df"] = df_step2_pending
                st.session_state["step2_logistics_col"] = col_lno_step2
                st.session_state["step2_alignment_report"] = report_step2
                st.session_state["step2_inbound_bytes"] = b_inbound
                st.session_state["step2_pending_bytes"] = b_pending
                st.session_state["step2_inbound_name"] = inbound_name
                st.session_state["step2_pending_name"] = pending_name

                append_operation_history(
                    stage="æ­¥éª¤äºŒå…¥åº“åŒ¹é…",
                    action="æ‰§è¡ŒåŒ¹é…",
                    detail={
                        "source_file": step2_source_label,
                        "inbound_file": inbound_source_label,
                        "input_rows": len(df_step2_source),
                        "output_rows": len(df_step2_matched),
                        "inbound_rows": len(df_step2_inbound),
                        "pending_rows": len(df_step2_pending),
                        "alignment_can_compare": report_step2.get("can_compare"),
                        "alignment_ok": report_step2.get("ok") if report_step2.get("can_compare") else None,
                        "alignment_missing_rows": report_step2.get("missing_rows"),
                        "alignment_extra_rows": report_step2.get("extra_rows"),
                        "artifacts": [p for p in [inbound_artifact, pending_artifact] if p],
                    }
                )

                st.success("âœ… æ­¥éª¤äºŒåŒ¹é…å®Œæˆã€‚å¯ç›´æ¥åœ¨ä¸‹æ–¹æ­¥éª¤ä¸‰ä½¿ç”¨â€œå·²å…¥åº“è¡¨â€å¯åŠ¨ AI å®¡æ ¸ã€‚")

            except ValueError as ve:
                st.error(f"âŒ æ­¥éª¤äºŒåŒ¹é…å¤±è´¥ï¼Œç¼ºå°‘å¿…è¦åˆ—ï¼š\n\n{ve}")
            except Exception as e:
                st.error(f"âŒ æ­¥éª¤äºŒåŒ¹é…å¤±è´¥ï¼š{e}")
                with st.expander("æŸ¥çœ‹é”™è¯¯è¯¦æƒ…ï¼ˆå¼€å‘ç”¨ï¼‰"):
                    st.code(traceback.format_exc())

    cached_step2_matched = st.session_state.get("step2_matched_df")
    cached_step2_inbound = st.session_state.get("step2_inbound_df")
    cached_step2_pending = st.session_state.get("step2_not_inbound_df")
    if isinstance(cached_step2_matched, pd.DataFrame) and isinstance(cached_step2_inbound, pd.DataFrame) and isinstance(cached_step2_pending, pd.DataFrame):
        st.caption(f"å½“å‰ä¼šè¯ç¼“å­˜çš„æ­¥éª¤äºŒå·²å…¥åº“æ•°æ®ï¼š{len(cached_step2_inbound)} æ¡ã€‚")

        c1, c2, c3 = st.columns(3)
        c1.metric("æ­¥éª¤äºŒåŒ¹é…æ€»è¡Œæ•°", len(cached_step2_matched))
        c2.metric("å·²å…¥åº“ï¼ˆå¯è¿›AIï¼‰", len(cached_step2_inbound))
        c3.metric("æœªå…¥åº“ï¼ˆå¾…è·Ÿè¿›ï¼‰", len(cached_step2_pending))

        render_preview_dataframe(
            cached_step2_inbound,
            title="âœ… å·²å…¥åº“ï¼ˆå¯è¿›å…¥AIå¤æ ¸ï¼‰",
            key_prefix="tab2_cached_inbound",
            default_rows=100,
            height=340,
            expanded=False,
        )
        render_preview_dataframe(
            cached_step2_pending,
            title="âš ï¸ æœªå…¥åº“ï¼ˆéœ€ç»§ç»­è·Ÿè¿›ï¼‰",
            key_prefix="tab2_cached_pending",
            default_rows=100,
            height=340,
            expanded=False,
        )

        b_inbound = st.session_state.get("step2_inbound_bytes")
        b_pending = st.session_state.get("step2_pending_bytes")
        inbound_name = st.session_state.get("step2_inbound_name", f"{now_ts()}_å…¥åº“åŒ¹é…é€šè¿‡_å¾…AIå¤æ ¸.xlsx")
        pending_name = st.session_state.get("step2_pending_name", f"{now_ts()}_æœªå…¥åº“å¾…è·Ÿè¿›.xlsx")

        if not b_inbound or not b_pending:
            shot_col_step2 = find_first_existing_column(cached_step2_matched, COL_SCREENSHOT_CANDIDATES)
            hyperlink_cols_inbound = [shot_col_step2] if shot_col_step2 and shot_col_step2 in cached_step2_inbound.columns else None
            hyperlink_cols_pending = [shot_col_step2] if shot_col_step2 and shot_col_step2 in cached_step2_pending.columns else None
            b_inbound = df_to_excel_bytes(cached_step2_inbound, sheet_name="å·²å…¥åº“", hyperlink_cols=hyperlink_cols_inbound)
            b_pending = df_to_excel_bytes(cached_step2_pending, sheet_name="æœªå…¥åº“", hyperlink_cols=hyperlink_cols_pending)

        dl1, dl2 = st.columns(2)
        with dl1:
            st.download_button(
                "â¬‡ï¸ ä¸‹è½½ï¼šå·²å…¥åº“è¡¨ï¼ˆå¾…AIå¤æ ¸ï¼‰",
                data=b_inbound,
                file_name=inbound_name,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        with dl2:
            st.download_button(
                "â¬‡ï¸ ä¸‹è½½ï¼šæœªå…¥åº“è¡¨ï¼ˆå¾…è·Ÿè¿›ï¼‰",
                data=b_pending,
                file_name=pending_name,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

        report_step2_cached = st.session_state.get("step2_alignment_report")
        if isinstance(report_step2_cached, dict):
            render_alignment_report(report_step2_cached, title="æ­¥éª¤äºŒæºæ•°æ® vs åŒ¹é…ç»“æœä¸€è‡´æ€§æ ¡éªŒ")

    st.divider()

    # -------------------------------
    # æ­¥éª¤ä¸‰ï¼ˆBï¼‰ï¼šAI å¤æ ¸ï¼ˆå¤šå›¾ï¼‰
    # -------------------------------
    st.subheader("æ­¥éª¤ä¸‰ï¼ˆBï¼‰ï¼šAI å¤æ ¸ï¼ˆå¤šå›¾ï¼‰")
    st.info("æ¨èï¼šç›´æ¥ä½¿ç”¨æ­¥éª¤äºŒäº§å‡ºçš„ã€å·²å…¥åº“è¡¨ã€‘è¿›è¡Œ AI å¤æ ¸ã€‚")

    left, right = st.columns([2, 1])
    with left:
        uploaded_2 = st.file_uploader(
            "ä¸Šä¼ â€œç¡®è®¤å…¥åº“åçš„æ­£å¸¸è¡¨â€ï¼ˆ.xlsx / .xls / .csvï¼‰",
            type=["xlsx", "xls", "csv"],
            key="tab3_uploader"
        )
    with right:
        use_step2_df = st.checkbox(
            "ç›´æ¥ä½¿ç”¨æ­¥éª¤äºŒçš„ã€å·²å…¥åº“è¡¨ã€‘ï¼ˆæ¨èï¼‰",
            value=True,
            key="use_step2_df_for_ai"
        )

    if "active_ai_task_id" not in st.session_state:
        st.session_state["active_ai_task_id"] = ""
    if "ai_task_auto_refresh" not in st.session_state:
        st.session_state["ai_task_auto_refresh"] = True

    df_in = None
    file_bytes_2 = b""
    source_file_label = ""
    create_ready = False
    col_amount2 = ""
    col_shot2 = ""

    if use_step2_df and isinstance(st.session_state.get("step2_inbound_df"), pd.DataFrame):
        df_in = st.session_state["step2_inbound_df"].copy()
        source_file_label = "æ­¥éª¤äºŒå·²å…¥åº“è¡¨ï¼ˆä¼šè¯ï¼‰"
        st.success("å·²è½½å…¥æ­¥éª¤äºŒå·²å…¥åº“è¡¨ï¼ˆå½“å‰ä¼šè¯ï¼‰ã€‚")
        render_preview_dataframe(
            df_in,
            title="å¾… AI å®¡æ ¸æ•°æ®é¢„è§ˆï¼ˆæ¥è‡ªæ­¥éª¤äºŒï¼‰",
            key_prefix="tab3_step2_session_df",
            default_rows=50,
            height=360,
            expanded=False,
        )
    elif uploaded_2 is not None:
        try:
            file_bytes_2 = get_uploaded_bytes(uploaded_2)
            df_in = read_table(uploaded_2)
            source_file_label = uploaded_2.name
            render_preview_dataframe(
                df_in,
                title="å·²è¯»å–å¾… AI å®¡æ ¸æ•°æ®é¢„è§ˆ",
                key_prefix="tab3_uploaded_df",
                default_rows=50,
                height=360,
                expanded=False,
            )
        except Exception as e:
            st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{e}")
            df_in = None

    current_task = None
    active_task_id = str(st.session_state.get("active_ai_task_id", "")).strip()
    if active_task_id:
        current_task = load_ai_task_state(active_task_id)
    if current_task is None:
        recovered_task = load_latest_ai_task_state(prefer_active=True)
        if recovered_task is not None:
            st.session_state["active_ai_task_id"] = recovered_task.get("task_id", "")
            current_task = recovered_task

    if current_task is not None:
        current_task = finalize_ai_task_if_needed(current_task)

    try:
        if df_in is not None and (not df_in.empty):
            required2 = {"é€€å›è¿è´¹é‡‘é¢": COL_AMOUNT_CANDIDATES, "å¯„å›è¿è´¹æˆªå›¾": COL_SCREENSHOT_CANDIDATES}
            matched2 = ensure_required_columns(df_in, required2)
            col_amount2 = matched2["é€€å›è¿è´¹é‡‘é¢"]
            col_shot2 = matched2["å¯„å›è¿è´¹æˆªå›¾"]

            # è‹¥ä¸Šä¼ çš„æ˜¯ Excelï¼ŒæŠ½å– screenshot åˆ— URLï¼ˆå…¼å®¹è¶…é“¾æ¥/å…¬å¼/tooltip/æ‰¹æ³¨ï¼‰
            if uploaded_2 is not None and uploaded_2.name.lower().endswith((".xlsx", ".xls")):
                df_in = attach_hyperlink_helper_column(df_in, file_bytes_2, col_shot2)
            create_ready = True

        if (df_in is None or df_in.empty) and current_task is None:
            st.info("è¯·å…ˆåœ¨æ­¥éª¤äºŒå®Œæˆå…¥åº“åŒ¹é…å¹¶ä½¿ç”¨å·²å…¥åº“è¡¨ï¼Œæˆ–æ‰‹åŠ¨ä¸Šä¼ ç¡®è®¤å…¥åº“åçš„æ­£å¸¸è¡¨ã€‚")

        ctrl1, ctrl2, ctrl3, ctrl4 = st.columns(4)
        with ctrl1:
            start_task = st.button(
                "ğŸš€ åˆ›å»ºå¹¶å¯åŠ¨ AI åå°ä»»åŠ¡",
                type="primary",
                key="start_ai_task_background",
                disabled=not create_ready
            )
        with ctrl2:
            pause_task = st.button(
                "â¸ï¸ æš‚åœä»»åŠ¡",
                key="pause_ai_task",
                disabled=not (isinstance(current_task, dict) and current_task.get("status") == AI_TASK_STATUS_RUNNING)
            )
        with ctrl3:
            resume_task = st.button(
                "â–¶ï¸ ç»§ç»­ä»»åŠ¡",
                key="resume_ai_task",
                disabled=not (isinstance(current_task, dict) and current_task.get("status") in (AI_TASK_STATUS_PAUSED, AI_TASK_STATUS_ERROR))
            )
        with ctrl4:
            refresh_task = st.button("ğŸ”„ åˆ·æ–°ä»»åŠ¡çŠ¶æ€", key="refresh_ai_task_status")

        st.checkbox("ä»»åŠ¡è¿è¡Œæ—¶è‡ªåŠ¨åˆ·æ–°è¿›åº¦ï¼ˆæ¯ç§’ï¼‰", key="ai_task_auto_refresh")

        if start_task:
            if not api_key_input:
                st.warning("æœªæ£€æµ‹åˆ° DashScope API Keyï¼šè¯·åœ¨ä¾§è¾¹æ è¾“å…¥æˆ–é…ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEYã€‚")
            elif isinstance(current_task, dict) and current_task.get("status") == AI_TASK_STATUS_RUNNING:
                st.warning("å½“å‰å·²æœ‰è¿è¡Œä¸­çš„ AI ä»»åŠ¡ï¼Œè¯·å…ˆæš‚åœåå†åˆ›å»ºæ–°ä»»åŠ¡ã€‚")
            elif not create_ready:
                st.warning("å½“å‰æ•°æ®æœªå°±ç»ªï¼Œæ— æ³•åˆ›å»º AI ä»»åŠ¡ã€‚")
            else:
                total_rows = min(len(df_in), int(max_ai_rows))
                new_task = create_ai_task_state(
                    df_source=df_in,
                    source_file=source_file_label,
                    col_amount=col_amount2,
                    col_shot=col_shot2,
                    total_rows=total_rows,
                    model_name=model_name,
                    max_images=int(max_images_per_row),
                    min_interval_sec=float(min_interval_sec),
                    max_retries=int(max_retries),
                    backoff_base_sec=float(backoff_base_sec),
                )
                st.session_state["active_ai_task_id"] = new_task.get("task_id", "")
                append_operation_history(
                    stage="æ­¥éª¤ä¸‰AIå¤æ ¸",
                    action="åˆ›å»ºAIä»»åŠ¡",
                    detail={
                        "task_id": new_task.get("task_id", ""),
                        "source_file": source_file_label,
                        "input_rows": len(df_in),
                        "processed_rows": 0,
                        "max_rows": total_rows,
                        "model": model_name,
                    }
                )
                if start_ai_task_worker(new_task.get("task_id", ""), api_key_input):
                    st.success(f"âœ… AI åå°ä»»åŠ¡å·²å¯åŠ¨ï¼š{new_task.get('task_id', '')}")
                else:
                    st.error("âŒ AI ä»»åŠ¡å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»»åŠ¡çŠ¶æ€åé‡è¯•ã€‚")
                current_task = load_ai_task_state(new_task.get("task_id", ""))

        if pause_task and isinstance(current_task, dict):
            current_task["status"] = AI_TASK_STATUS_PAUSED
            current_task["worker_token"] = ""
            save_ai_task_state(current_task)
            append_operation_history(
                stage="æ­¥éª¤ä¸‰AIå¤æ ¸",
                action="æš‚åœAIä»»åŠ¡",
                detail={
                    "task_id": current_task.get("task_id", ""),
                    "processed_rows": int(current_task.get("next_idx", 0)),
                    "max_rows": int(current_task.get("total", 0)),
                }
            )
            st.info("ä»»åŠ¡å·²æš‚åœã€‚")
            current_task = load_ai_task_state(current_task.get("task_id", ""))

        if resume_task and isinstance(current_task, dict):
            if not api_key_input:
                st.warning("ç»§ç»­ä»»åŠ¡éœ€è¦ DashScope API Keyï¼Œè¯·å…ˆåœ¨ä¾§è¾¹æ è¾“å…¥ã€‚")
            else:
                if start_ai_task_worker(current_task.get("task_id", ""), api_key_input):
                    append_operation_history(
                        stage="æ­¥éª¤ä¸‰AIå¤æ ¸",
                        action="ç»§ç»­AIä»»åŠ¡",
                        detail={
                            "task_id": current_task.get("task_id", ""),
                            "processed_rows": int(current_task.get("next_idx", 0)),
                            "max_rows": int(current_task.get("total", 0)),
                        }
                    )
                    st.success("ä»»åŠ¡å·²ç»§ç»­æ‰§è¡Œã€‚")
                else:
                    st.error("âŒ ä»»åŠ¡ç»§ç»­å¤±è´¥ï¼Œè¯·é‡è¯•ã€‚")
                current_task = load_ai_task_state(current_task.get("task_id", ""))

        if refresh_task and isinstance(current_task, dict):
            current_task = load_ai_task_state(current_task.get("task_id", ""))

        if isinstance(current_task, dict):
            current_task = finalize_ai_task_if_needed(current_task)
            summary = summarize_ai_task(current_task)
            frames = split_ai_task_frames(current_task)

            st.markdown(f"#### ğŸ§  å½“å‰ä»»åŠ¡ï¼š`{current_task.get('task_id', '')}`ï¼ˆ{task_status_label(current_task.get('status', ''))}ï¼‰")
            if current_task.get("error_message"):
                st.warning(f"ä»»åŠ¡æç¤ºï¼š{current_task.get('error_message')}")

            m1, m2, m3, m4, m5 = st.columns(5)
            m1.metric("è®¡åˆ’å¤„ç†", summary["total"])
            m2.metric("å·²å¤„ç†", summary["processed_rows"])
            m3.metric("æœªå¤„ç†", summary["pending_rows"])
            m4.metric("å¤„ç†æ­£å¸¸", summary["ok_rows"])
            m5.metric("å¤„ç†å¼‚å¸¸", summary["bad_rows"])

            total = max(1, int(summary["total"]))
            progress_ratio = min(1.0, float(summary["processed_rows"]) / float(total))
            st.progress(progress_ratio)
            st.caption(
                f"è¿›åº¦ï¼š{summary['processed_rows']}/{summary['total']} | "
                f"åˆ›å»ºæ—¶é—´ï¼š{current_task.get('created_at', '')} | "
                f"æœ€è¿‘æ›´æ–°ï¼š{current_task.get('updated_at', '')}"
            )

            col_shot_task = str(current_task.get("col_shot", ""))
            hyperlink_ok = [col_shot_task] if col_shot_task and col_shot_task in frames["ok"].columns else None
            hyperlink_bad = [col_shot_task] if col_shot_task and col_shot_task in frames["bad"].columns else None
            hyperlink_pending = [col_shot_task] if col_shot_task and col_shot_task in frames["pending"].columns else None

            b_ok_partial = df_to_excel_bytes(frames["ok"], sheet_name="AIå·²å¤„ç†æ­£å¸¸", hyperlink_cols=hyperlink_ok)
            b_bad_partial = df_to_excel_bytes(frames["bad"], sheet_name="AIå·²å¤„ç†å¼‚å¸¸", hyperlink_cols=hyperlink_bad)
            b_pending_partial = df_to_excel_bytes(frames["pending"], sheet_name="AIæœªå¤„ç†", hyperlink_cols=hyperlink_pending)

            dl1, dl2, dl3 = st.columns(3)
            with dl1:
                st.download_button(
                    "â¬‡ï¸ ä¸‹è½½å·²å¤„ç†æ­£å¸¸",
                    data=b_ok_partial,
                    file_name=f"{now_ts()}_{current_task.get('task_id', '')}_AIå·²å¤„ç†æ­£å¸¸.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
            with dl2:
                st.download_button(
                    "â¬‡ï¸ ä¸‹è½½å·²å¤„ç†å¼‚å¸¸",
                    data=b_bad_partial,
                    file_name=f"{now_ts()}_{current_task.get('task_id', '')}_AIå·²å¤„ç†å¼‚å¸¸.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
            with dl3:
                st.download_button(
                    "â¬‡ï¸ ä¸‹è½½æœªå¤„ç†éƒ¨åˆ†",
                    data=b_pending_partial,
                    file_name=f"{now_ts()}_{current_task.get('task_id', '')}_AIæœªå¤„ç†.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

            with st.expander("æŸ¥çœ‹å·²å¤„ç†æ˜ç»†ï¼ˆæœ€è¿‘100æ¡ï¼‰", expanded=False):
                if frames["processed"].empty:
                    st.info("æš‚æ— å·²å¤„ç†è®°å½•ã€‚")
                else:
                    st.dataframe(frames["processed"].tail(100), use_container_width=True, height=360)

            with st.expander("æŸ¥çœ‹æœªå¤„ç†æ˜ç»†ï¼ˆæœ€å¤š100æ¡ï¼‰", expanded=False):
                if frames["pending"].empty:
                    st.info("å½“å‰æ²¡æœ‰æœªå¤„ç†è®°å½•ã€‚")
                else:
                    st.dataframe(frames["pending"].head(100), use_container_width=True, height=360)

            align_rep = current_task.get("alignment_report")
            if isinstance(align_rep, dict):
                render_alignment_report(align_rep, title="æ­¥éª¤ä¸‰æºæ•°æ® vs AIç»“æœä¸€è‡´æ€§æ ¡éªŒ")

            if current_task.get("status") == AI_TASK_STATUS_RUNNING and st.session_state.get("ai_task_auto_refresh", True):
                time.sleep(1.0)
                st.rerun()

    except ValueError as ve:
        st.error(f"âŒ è¡¨æ ¼ç¼ºå°‘å¿…è¦åˆ—ï¼Œè¯·æ£€æŸ¥ï¼š\n\n{ve}")
    except Exception as e:
        st.error(f"âŒ AI å®¡æ ¸æµç¨‹å¤±è´¥ï¼š{e}")
        with st.expander("æŸ¥çœ‹é”™è¯¯è¯¦æƒ…ï¼ˆå¼€å‘ç”¨ï¼‰"):
            st.code(traceback.format_exc())


# =============================================================================
# Tab1ï¼šæ¸…æ´—ä¸è§„åˆ™åˆç­›
# =============================================================================
with main_tabs[1]:
    st.subheader("æ­¥éª¤ä¸€ï¼šä¸Šä¼ ç­ç‰›ã€Šé€€è¿è´¹è‡ªåŠ©ç™»è®°è¡¨ã€‹å¹¶è¿›è¡Œè§„åˆ™åˆç­›")

    uploaded_1 = st.file_uploader("ä¸Šä¼ ã€Šé€€è¿è´¹è‡ªåŠ©ç™»è®°è¡¨ã€‹ï¼ˆ.xlsx / .xls / .csvï¼‰", type=["xlsx", "xls", "csv"], key="tab1_uploader")

    if uploaded_1 is None:
        st.info("è¯·å…ˆä¸Šä¼ ç­ç‰›å¯¼å‡ºçš„ç™»è®°è¡¨ã€‚")
    else:
        try:
            file_bytes_1 = get_uploaded_bytes(uploaded_1)
            df_raw = read_table(uploaded_1)

            if df_raw.empty:
                st.warning("è¯»å–åˆ°çš„è¡¨æ ¼ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹ã€‚")
            else:
                render_preview_dataframe(
                    df_raw,
                    title="å·²è¯»å–æ•°æ®é¢„è§ˆ",
                    key_prefix="tab1_uploaded_raw",
                    default_rows=50,
                    height=420,
                    expanded=False,
                )

                required = {
                    "é€€å›è¿è´¹é‡‘é¢": COL_AMOUNT_CANDIDATES,
                    "æ”¯ä»˜å®è´¦å·": COL_ALIPAY_ACCOUNT_CANDIDATES,
                    "æ”¯ä»˜å®å®å": COL_ALIPAY_NAME_CANDIDATES,
                    "é€€å›ç‰©æµå•å·": COL_LOGISTICS_NO_CANDIDATES,
                }
                matched_cols = ensure_required_columns(df_raw, required)

                col_amount = matched_cols["é€€å›è¿è´¹é‡‘é¢"]
                col_account = matched_cols["æ”¯ä»˜å®è´¦å·"]
                col_name = matched_cols["æ”¯ä»˜å®å®å"]
                col_lno = matched_cols["é€€å›ç‰©æµå•å·"]

                shot_col = find_first_existing_column(df_raw, COL_SCREENSHOT_CANDIDATES)

                # âœ… Excel æˆªå›¾åˆ— URL æŠ½å–ï¼ˆå…¼å®¹ hyperlink / å…¬å¼ / tooltip / æ‰¹æ³¨ï¼‰
                if shot_col and uploaded_1.name.lower().endswith((".xlsx", ".xls")):
                    df_raw = attach_hyperlink_helper_column(df_raw, file_bytes_1, shot_col)

                # æ ¡éªŒ
                df = df_raw.copy()
                validation_results = [
                    validate_row(
                        amount=amount,
                        alipay_account=account,
                        alipay_name=name,
                        logistics_no=logistics_no,
                    )
                    for amount, account, name, logistics_no in zip(
                        df[col_amount].tolist(),
                        df[col_account].tolist(),
                        df[col_name].tolist(),
                        df[col_lno].tolist(),
                    )
                ]
                flags = [ok for ok, _ in validation_results]
                reasons = [reason for _, reason in validation_results]

                df[COL_ABNORMAL_REASON] = reasons
                valid_mask = pd.Series(flags, index=df.index)
                df_normal = df[valid_mask].drop(columns=[COL_ABNORMAL_REASON], errors="ignore").copy()
                df_abnormal = df[~valid_mask].copy()
                report_step1 = compare_source_and_processed(df_raw, df, stage_name="æ­¥éª¤ä¸€æ¸…æ´—")

                c1, c2, c3 = st.columns(3)
                c1.metric("æ€»è¡Œæ•°", len(df))
                c2.metric("æ­£å¸¸ï¼ˆå¯ç»§ç»­åæŸ¥ï¼‰", len(df_normal))
                c3.metric("å¼‚å¸¸ï¼ˆéœ€å›è®¿ï¼‰", len(df_abnormal))

                render_preview_dataframe(
                    df_normal,
                    title="âœ… æ­£å¸¸è¡¨",
                    key_prefix="tab1_normal_df",
                    default_rows=100,
                    height=420,
                    expanded=False,
                )
                render_preview_dataframe(
                    df_abnormal,
                    title="âš ï¸ å¼‚å¸¸è¡¨",
                    key_prefix="tab1_abnormal_df",
                    default_rows=100,
                    height=420,
                    expanded=False,
                )

                # session ä¾› Tab2 ç›´æ¥ç”¨
                st.session_state["tab1_normal_df"] = df_normal
                st.session_state["tab1_abnormal_df"] = df_abnormal

                # å¯¼å‡ºï¼šæˆªå›¾åˆ—ä¿æŒâ€œé¢„è§ˆ/æµè§ˆâ€ç­‰åŸæ–‡å­—ï¼Œä½†å¯ç‚¹å‡»
                hyperlink_cols_normal = [shot_col] if shot_col and shot_col in df_normal.columns else None
                hyperlink_cols_abnormal = [shot_col] if shot_col and shot_col in df_abnormal.columns else None

                ts = now_ts()
                normal_name = f"{ts}_æ¸…æ´—æ­£å¸¸å¯ç»§ç»­åæŸ¥.xlsx"
                abnormal_name = f"{ts}_é€€è¿è´¹ä¿¡æ¯å¼‚å¸¸éœ€å›è®¿.xlsx"

                b1 = df_to_excel_bytes(df_normal, sheet_name="æ­£å¸¸", hyperlink_cols=hyperlink_cols_normal)
                b2 = df_to_excel_bytes(df_abnormal, sheet_name="å¼‚å¸¸", hyperlink_cols=hyperlink_cols_abnormal)

                col_dl1, col_dl2 = st.columns(2)
                with col_dl1:
                    st.download_button(
                        "â¬‡ï¸ ä¸‹è½½æ­£å¸¸è¡¨ï¼ˆå¯ç»§ç»­åæŸ¥ï¼‰",
                        data=b1,
                        file_name=normal_name,
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
                with col_dl2:
                    st.download_button(
                        "â¬‡ï¸ ä¸‹è½½å¼‚å¸¸è¡¨ï¼ˆéœ€å›è®¿ï¼‰",
                        data=b2,
                        file_name=abnormal_name,
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

                render_alignment_report(report_step1, title="æ­¥éª¤ä¸€æºæ•°æ® vs æ¸…æ´—ç»“æœä¸€è‡´æ€§æ ¡éªŒ")

                step1_history_key = f"{uploaded_1.name}|{len(file_bytes_1)}|{len(df)}|{len(df.columns)}"
                if st.session_state.get("step1_last_history_key") != step1_history_key:
                    normal_artifact = save_artifact_bytes("step1_normal", normal_name, b1)
                    abnormal_artifact = save_artifact_bytes("step1_abnormal", abnormal_name, b2)
                    append_operation_history(
                        stage="æ­¥éª¤ä¸€æ¸…æ´—",
                        action="æ‰§è¡Œæ¸…æ´—",
                        detail={
                            "source_file": uploaded_1.name,
                            "input_rows": len(df_raw),
                            "output_rows": len(df),
                            "normal_rows": len(df_normal),
                            "abnormal_rows": len(df_abnormal),
                            "alignment_can_compare": report_step1.get("can_compare"),
                            "alignment_ok": report_step1.get("ok") if report_step1.get("can_compare") else None,
                            "alignment_missing_rows": report_step1.get("missing_rows"),
                            "alignment_extra_rows": report_step1.get("extra_rows"),
                            "artifacts": [p for p in [normal_artifact, abnormal_artifact] if p],
                        }
                    )
                    st.session_state["step1_last_history_key"] = step1_history_key

        except ValueError as ve:
            st.error(f"âŒ è¡¨æ ¼ç¼ºå°‘å¿…è¦åˆ—ï¼Œè¯·æ£€æŸ¥åé‡æ–°å¯¼å‡º/ä¸Šä¼ ï¼š\n\n{ve}")
        except Exception as e:
            st.error(f"âŒ å¤„ç†å¤±è´¥ï¼š{e}")
            with st.expander("æŸ¥çœ‹é”™è¯¯è¯¦æƒ…ï¼ˆå¼€å‘ç”¨ï¼‰"):
                st.code(traceback.format_exc())
